{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef3266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def initialize_parameters(\n",
    "    n0: int, n1: int, n2: int, scale: float\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Initialize parameters for a 2-layer neural network.\n",
    "    Args:\n",
    "        n0 (int): Number of input features (aka nx)\n",
    "        n1 (int): Number of neurons in layer 1\n",
    "        n2 (int): Number of output neurons\n",
    "        scale (float): Scaling factor for parameters\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: weights and biases for 2 layers\n",
    "            W1 : (n1, n0)\n",
    "            b1 : (n1)\n",
    "            W2 : (n2, n1)\n",
    "            b2 : (n2)\n",
    "    \"\"\"\n",
    "    # TODO: initialize and return W1, b1, W2, and b2\n",
    "    W1 = torch.randn(n1, n0)\n",
    "    b1 = torch.randn(n1)\n",
    "    \n",
    "    W2 = torch.randn(n2, n1)\n",
    "    b2 = torch.randn(n2)\n",
    "    \n",
    "    return Tuple[W1,b1,W2,b2]\n",
    "\n",
    "\n",
    "def forward_propagation(\n",
    "    A0: Tensor, W1: Tensor, b1: Tensor, W2: Tensor, b2: Tensor\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Compute the output of a 2-layer neural network.\n",
    "    Args:\n",
    "        A0 (Tensor): (N, n0) input matrix (aka X)\n",
    "        W1 (Tensor): (n1, n0) layer 1 weight matrix\n",
    "        b1 (Tensor): (n1) layer 1 bias matrix\n",
    "        W2 (Tensor): (n2, n1) layer 2 weight matrix\n",
    "        b2 (Tensor): (n2) layer 2 bias matrix\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: outputs for layers 1 (N, n1) and 2 (N, n2)\n",
    "    \"\"\"\n",
    "    # TODO: compute and return A1 and A2\n",
    "    W1Auto = W1.clone().detach().requires_grad_(True)\n",
    "    b1Auto = b1.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    W2Auto = W2.clone().detach().requires_grad_(True)\n",
    "    b1Auto = b2.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    #given A0, W1, b1, W2, b2\n",
    "    Z1 = torch.nn.functional.linear(A0, W1Auto, b1Auto)\n",
    "    A1 = torch.sigmoid(Z1)\n",
    "    \n",
    "    Z2 = torch.nn.functional.linear(A1, W2Auto, b2Auto)\n",
    "    A2 = torch.sigmoid(Z2)\n",
    "    \n",
    "    return Tuple[A1, A2]\n",
    "\n",
    "\n",
    "def sigmoid_to_binary(A2: Tensor) -> Tensor:\n",
    "    \"\"\"Convert the output of a final layer sigmoids to zeros and ones.\n",
    "    Args:\n",
    "        A2 (Tensor): (N, n2) output of the network\n",
    "    Returns:\n",
    "        Tensor: binary predictions of a 2-layer neural network\n",
    "    \"\"\"\n",
    "    # TODO: convert matrix to rounded zeros and ones\n",
    "    return 1 / (1 + torch.exp(A2))\n",
    "\n",
    "\n",
    "def backward_propagation(\n",
    "    A0: Tensor, A1: Tensor, A2: Tensor, Y: Tensor, W2: Tensor\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Compute gradients of a 2-layer neural network's parameters.\n",
    "    Args:\n",
    "        A0 (Tensor): (N, n0) input matrix (aka X)\n",
    "        A1 (Tensor): (N, n1) output of layer 1 from forward propagation\n",
    "        A2 (Tensor): (N, n2) output of layer 2 from forward propagation (aka Yhat)\n",
    "        Y (Tensor): (N, n2) correct targets (aka targets)\n",
    "        W2 (Tensor): (n2, n1) weight matrix\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: gradients for weights and biases\n",
    "    \"\"\"\n",
    "    # TODO: compute and return gradients\n",
    "    Yhat = A2\n",
    "    bce_loss = -torch.nn.functional.binary_cross_entropy(Yhat, Y)\n",
    "    bce_loss.backward()\n",
    "\n",
    "\n",
    "def update_parameters(\n",
    "    W1: Tensor,\n",
    "    b1: Tensor,\n",
    "    W2: Tensor,\n",
    "    b2: Tensor,\n",
    "    dW1: Tensor,\n",
    "    db1: Tensor,\n",
    "    dW2: Tensor,\n",
    "    db2: Tensor,\n",
    "    lr: float,\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Update parameters of a 2-layer neural network.\n",
    "    Args:\n",
    "        W1 (Tensor): (n1, n0) weight matrix\n",
    "        b1 (Tensor): (n1) bias matrix)\n",
    "        W2 (Tensor): (n2, n1) weight matrix)\n",
    "        b2 (Tensor): (n2) bias matrix\n",
    "        dW1 (Tensor): (n1, n0) gradient matrix\n",
    "        db1 (Tensor): (n1) gradient matrix)\n",
    "        dW2 (Tensor): (n2, n1) gradient matrix)\n",
    "        db2 (Tensor): (n2) gradient matrix\n",
    "        lr (float): learning rate\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: updated network parameters\n",
    "    \"\"\"\n",
    "    # TODO: Update and return parameters\n",
    "\n",
    "\n",
    "def compute_loss(A2: Tensor, Y: Tensor) -> Tensor:\n",
    "    \"\"\"Compute mean loss using binary cross entropy loss.\n",
    "    Args:\n",
    "        A2 (Tensor): (N, n2) matrix of neural network output values (aka Yhat)\n",
    "        Y (Tensor): (N, n2) correct targets (aka targets)\n",
    "    Returns:\n",
    "        Tensor: computed loss\n",
    "    \"\"\"\n",
    "    # TODO: implement this function\n",
    "\n",
    "\n",
    "def train_2layer(\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    num_hidden: int,\n",
    "    param_scale: float,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"A function for performing batch gradient descent with a 2-layer network.\n",
    "    Args:\n",
    "        X (Tensor): (N, nx) matrix of input features\n",
    "        Y (Tensor): (N, ny) matrix of correct targets (aka targets)\n",
    "        num_hidden (int): number of neurons in layer 1\n",
    "        param_scale (float): scaling factor for initializing parameters\n",
    "        num_epochs (int): number of training passes through all data\n",
    "        learning_rate (float): learning rate\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: learned parameters of a 2-layer neural network\n",
    "    \"\"\"\n",
    "    # TODO: implement this function\n",
    "    # Steps:\n",
    "    # 1. create and initialize parameters\n",
    "    # 2. loop\n",
    "    #   1. compute outputs with forward propagation\n",
    "    #   2. compute loss (for analysis)\n",
    "    #   3. compute gradients with backward propagation\n",
    "    #   4. update parameters\n",
    "    # 3. return final parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cafb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

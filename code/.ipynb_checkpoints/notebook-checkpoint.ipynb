{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disaster Types\n",
    "- Year\n",
    "- State\n",
    "- Households Inflow (Number of Returns)\n",
    "- Households Outflow (Number of Returns)\n",
    "- Individuals Inflow (Number of Exemptions)\n",
    "- Individuals Outflow (Number of Exemptions)\n",
    "- Chemical\n",
    "- Dam/Levee Break\n",
    "- Drought\n",
    "- Earthquake\n",
    "- Fire\n",
    "- Flood\n",
    "- Human Cause\n",
    "- Hurricane\n",
    "- Ice\n",
    "- Mud/Landslide\n",
    "- Other\n",
    "- Snow\n",
    "- Storm\n",
    "- Terrorism\n",
    "- Tornado\n",
    "- Tsunami\n",
    "- Typhoon\n",
    "- Volcano\n",
    "- Water\n",
    "- Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Folders Setup\n",
    "\n",
    "code \n",
    "    notebook.ipynb\n",
    "    data\n",
    "        Disasters\n",
    "            FEMA_dataset.csv\n",
    "        StateMigration\n",
    "            1990to1991StateMigration\n",
    "                 1990to1991StateMigrationInflow\n",
    "                     Alabama91in.xls\n",
    "                     Alaska91in.xls\n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91in.xls\n",
    "                     Wyoming91in.xls \n",
    "                 1990to1991StateMigrationOutflow\n",
    "                     Alabama91Out.xls\n",
    "                     Alaska91Out.xls \n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91Out.xls\n",
    "                     Wyoming91Out.xls \n",
    "            .\n",
    "            .\n",
    "            .\n",
    "            2008to2009StateMigration\n",
    "            2009to2010StateMigration\n",
    "            2010to2011StateMigration \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode:\n",
    "\n",
    "### FEMA Dataset Pre-processing (Neely)\n",
    "1. Create new FEMA_dataset with columns \n",
    "    - contains Year, State, Disaster Type\n",
    "2. Name file \"State_Disasters_by_Year\"\n",
    "\n",
    "### StateMigration Data Pre-Processing (Ben)\n",
    "1. Convert all datasets in StateMigration from .xls into .csv files\n",
    "2. Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\" - assign I if from inflow and O if from outflow - from every state file.\n",
    "3. Extract \"State\" and \"Year\" from every file\n",
    "4. Create file with \"State\" (from file name), \"Year\" (from file name), \"Number_of_Returns_I\", \"Number_of_Exemptions_I\", \"Number_of_Returns_I\" and \"Number_of_Exemptions_O\"\n",
    "5. Name file \"State Migration by Year\"\n",
    "\n",
    "### Merge Datasets (Both)\n",
    "1. Merge datasets on common attributes \"Year\" and \"State\"\n",
    "2. Name dataset \"State_Migration_and_Disasters_by_Year\"\n",
    "\n",
    "#### Train and Testing\n",
    "1. Create training and testing datasets\n",
    "    Questions: How should we split training and testing data?\n",
    "2. Create Neural Network models\n",
    "    Input: Year, State, Disaster Type\n",
    "    Output: Migration Inflow (Household/Individual), Migration Outflow (Household/Individual)\n",
    "3. Put training and testing through the Neural Network models.\n",
    "4. Evaluate which models are the most effective.\n",
    "---\n",
    "Data Augmentation\n",
    "Synthetic Data\n",
    "Use svm or decision tree -- skleant -- and compare against a neural network\n",
    "could use all data for training and all data for validation - not this is a faulty practice in \n",
    "20% distribution of state \n",
    "\n",
    "\n",
    "```\n",
    "read_file = pd.read_excel (\"Test.xlsx\")\n",
    " \n",
    "# Write the dataframe object\n",
    "# into csv file\n",
    "read_file.to_csv (\"Test.csv\",\n",
    "                  index = None,\n",
    "                  header=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding imports\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import xlrd\n",
    "import csv\n",
    "import numpy as np\n",
    "#from fastai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEMA Dataset Pre-processing\n",
    "\n",
    "Creates State_Disasters_by_Year.csv with:\n",
    "- State\n",
    "- Disaster Type\n",
    "- Start Year\n",
    "- End Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMA Dataset Preprocessing\n",
    "\n",
    "# copy original FEMA dataset to new file\n",
    "original = r'../code/data/Disasters/FEMA_dataset.csv'\n",
    "new = r'../code/data/Disasters/State_Disasters_by_Year.csv'\n",
    "shutil.copyfile(original, new)\n",
    "\n",
    "# read csv file\n",
    "data = pd.read_csv('~/code/data/Disasters/State_Disasters_by_Year.csv')\n",
    "\n",
    "# delete irrelevant rows\n",
    "data.pop('Declaration Number')\n",
    "data.pop('Declaration Type')\n",
    "data.pop('Declaration Date')\n",
    "data.pop('County')\n",
    "data.pop('Disaster Title')\n",
    "data.pop('Close Date')\n",
    "data.pop('Individual Assistance Program')\n",
    "data.pop('Public Assistance Program')\n",
    "data.pop('Hazard Mitigation Program')\n",
    "data.pop('Individuals & Households Program')\n",
    "\n",
    "# extract years\n",
    "data['Start Year'] = pd.DatetimeIndex(data['Start Date']).year\n",
    "data['End Year'] = pd.DatetimeIndex(data['End Date']).year\n",
    "\n",
    "# delete start and end dates\n",
    "data.pop('Start Date')\n",
    "data.pop('End Date')\n",
    "\n",
    "print(data)\n",
    "\n",
    "# save changes csv\n",
    "data.to_csv('../code/data/Disasters/State_Disasters_by_Year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting State Migration data from .xls to .csv\n",
    "\n",
    "Converting all datasets in StateMigration from .xls to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all files in StateMigration folder from .xls to .csv\n",
    "\n",
    "# create list of xls files\n",
    "xls_list = glob.glob(\"/Users/ben/Desktop/climate-displacement/code/data/StateMigration/*/*/*.xls\")\n",
    "\n",
    "# replace xls \n",
    "for xls_file in xls_list:\n",
    "    \n",
    "    wb = xlrd.open_workbook(xls_file)\n",
    "    sh = wb.sheet_by_index(0)\n",
    "    csv_file = open(xls_file[0:-3]+'csv', \"w\")\n",
    "    wr = csv.writer(csv_file, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    for rownum in range(sh.nrows):\n",
    "        wr.writerow(sh.row_values(rownum))\n",
    "        \n",
    "    csv_file.close()\n",
    "    \n",
    "    # remove .xls files\n",
    "    os.remove(xls_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More data wrangling - StateMigration dataset\n",
    "- Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\"\n",
    "- Assign \"I\" if from inflow and \"O\" if from outflow - for every state file\n",
    "- Extract \"State\" and \"Year\" from every file\n",
    "- Create file with \"State\" (from file name), \"Year\" (from file name), \"Number of Returns_I\", \"Number of Exemptions I\", \"Number of Returns O\" and \"Number of Exemptions O\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "1. create output path in repository for the merged StateMigration dataset\n",
    "\n",
    "2. read each csv file in the StateMigration folder, and for each file, \n",
    "    - d\n",
    "3. check the csv files to make sure they are the intended data\n",
    "4. remove the original csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new file location for merged StateMigration dataset\n",
    "output_path = r'../code/data/StateMigration/State_Migrations_by_Year.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output file at output_path\n",
    "output = open(output_path, \"w\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty DataFrame object\n",
    "df = pd.DataFrame()\n",
    "df.insert(0,'State', '')\n",
    "df.insert(1,'Year', '')\n",
    "df.insert(2,'NOR(I)', '')\n",
    "df.insert(3,'NOE(I)', '')\n",
    "df.insert(4,'NOR(O)', '')\n",
    "df.insert(5,'NOE(O)', '')\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WI\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of \"state initial keys\" with multiple \"values\"\n",
    "# run each segment through dictionary, and convert into state initial\n",
    "stateDict = {\n",
    "    \"AL\":['Alabama', 'al', 'AL', 'alab', 'Alab'],\n",
    "    \"AK\":['Alaska', 'ak', 'AK', 'alas', 'Alas'],\n",
    "    \"AZ\":['Arizona', 'az', 'AZ', 'ariz', 'Ariz'],\n",
    "    \"AR\":['Arkansas', 'ar', 'AR', 'arka', 'Arka', 'aka'],\n",
    "    \"CA\":['California', 'ca', 'CA', 'cali', 'Cali'],\n",
    "    \"CO\":['Colorado', 'co', 'CO', 'colo', 'Colo'],\n",
    "    \"CT\":['Connecticut', 'ct', 'CT', 'conn', 'Conn'],\n",
    "    \"DE\":['Delaware', 'de', 'DE', 'dela', 'Dela'],\n",
    "    \"DC\":['DistrictofColumbia', 'Districtofcolumbia', 'District of Columbia', 'dc', 'DC', 'dist', 'Dist', 'DiCo', 'dico'],\n",
    "    \"FL\":['Florida', 'fl', 'FL', 'flor', 'Flor'],\n",
    "    \"GA\":['Georgia', 'ga', 'GA', 'geor', 'Geor'],\n",
    "    \"HI\":['Hawaii', 'hi', 'HI', 'hawa', 'Hawa'],\n",
    "    \"ID\":['Idaho', 'id', 'ID', 'idah', 'Idah'],\n",
    "    \"IL\":['Illinois', 'il', 'IL', 'illi', 'Illi'],\n",
    "    \"IN\":['Indiana', 'in', 'IN', 'indi', 'Indi'],\n",
    "    \"IA\":['Iowa', 'ia', 'IA', 'iowa'],\n",
    "    \"KS\":['Kansas', 'ks', 'KS', 'kans', 'Kans'],\n",
    "    \"KY\":['Kentucky', 'ky', 'KY', 'kent', 'Kent'],\n",
    "    \"LA\":['Louisiana', 'la', 'LA', 'loui', 'Loui'],\n",
    "    \"MA\":['Massachusetts', 'ma', 'MA', 'mass', 'Mass'],\n",
    "    \"MD\":['Maryland', 'md', 'MD', 'mary', 'Mary'],\n",
    "    \"ME\":['Maine', 'me', 'ME', 'main', 'Main'],\n",
    "    \"MI\":['Michigan', 'mi', 'MI', 'mich', 'Mich'],\n",
    "    \"MN\":['Minnesota', 'mn', 'MN', 'minn', 'Minn'],\n",
    "    \"MO\":['Missouri', 'mo', 'MO', 'Miso', 'miso'],\n",
    "    \"MS\":['Mississippi', 'ms', 'MS', 'Misi', 'misi', 'miss', 'Miss'],\n",
    "    \"MT\":['Montana', 'mt', 'MT', 'mont', 'Mont'],\n",
    "    \"NC\":['North Carolina', 'NorthCarolina', 'nc', 'NC', 'NoCa', 'noca', 'ncar', 'Northcarolina'],\n",
    "    \"ND\":['North Dakota', 'NorthDakota', 'nd', 'ND', 'NoDa', 'noda', 'ndak', 'Northdakota'],\n",
    "    \"NE\":['Nebraska', 'ne', 'NE', 'Nebr', 'nrbt', 'nebr'],\n",
    "    \"NH\":['New Hampshire', 'NewHampshire', 'nh', 'NH', 'NeHa', 'neha', 'newh'],\n",
    "    \"NJ\":['New Jersey', 'NewJersey', 'nj', 'NJ', 'NeJe', 'neje', 'newj', 'Newjersey'],\n",
    "    \"NM\":['New Mexico', 'NewMexico', 'nm', 'NM', 'NeMe', 'neme', 'newm', 'Newmexico'],\n",
    "    \"NV\":['Nevada', 'nv', 'NV', 'Neva', 'neva'],\n",
    "    \"NY\":['New York', 'NewYork', 'ny', 'NY', 'newy', 'NeYo', 'neyo', 'newY','Newyork'],\n",
    "    \"OH\":['Ohio', 'oh', 'OH', 'ohio', 'nhio'],\n",
    "    \"OK\":['Oklahoma', 'ok', 'OK', 'okla', 'Okla'],\n",
    "    \"OR\":['Oregon', 'or', 'OR', 'oreg', 'Oreg', 'oeg'],\n",
    "    \"PA\":['Pennsylvania', 'pa', 'PA', 'penn', 'Penn'],\n",
    "    \"RI\":['Rhode Island', 'RhodeIsland', 'ri', 'RI', 'Rhls', 'rhod', 'Rhod', 'RhIs'],\n",
    "    \"SC\":['South Carolina', 'SouthCarolina', 'sc', 'SC', 'SoCa', 'soca', 'scar', 'Southcarolina'],\n",
    "    \"SD\":['South Dakota', 'SouthDakota', 'sd', 'SD', 'SoDa', 'soda', 'sdak', 'Southdakota'],\n",
    "    \"TN\":['Tennessee', 'tn', 'TN', 'Tenn', 'tenn'],\n",
    "    \"TX\":['Texas', 'tx', 'TX', 'texa', 'Texa'],\n",
    "    \"UT\":['Utah', 'ut', 'UT', 'utah'],\n",
    "    \"VA\":['Virginia', 'va', 'VA', 'virg', 'Virg', 'vrg'],\n",
    "    \"VT\":['Vermont', 'vt', 'VT', 'verm', 'Verm'],\n",
    "    \"WA\":['Washington', 'wa', 'WA', 'wash', 'Wash'],\n",
    "    \"WI\":['Wisconsin', 'wi', 'WI', 'wisc', 'Wisc', 'wiso', 'wsc'],\n",
    "    \"WV\":['West Virginia', 'WestVirginia', 'wv', 'WV', 'west', 'wevi', 'wvir', 'Westvirginia'],\n",
    "    \"WY\":['Wyoming', 'wy', 'WY', 'wyom', 'Wyom']    \n",
    "}\n",
    "\n",
    "def getKey(val):\n",
    "    for key, valueList in stateDict.items():\n",
    "         for value in valueList:\n",
    "            if val == value:\n",
    "                 return key\n",
    "    \n",
    "    return False\n",
    "\n",
    "print(getKey('Wisconsin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read csv file\n",
    "csv_list = glob.glob(\"/Users/ben/Desktop/climate-displacement/code/data/StateMigration/*/*/*.csv\")\n",
    "\n",
    "# create list of row_params\n",
    "list_param = []\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    # os.path.split returns a list of (head, tail) where head is the parent directories \n",
    "    # and tail is the filename and extension\n",
    "    temp = os.path.split(csv_file)\n",
    "    temp2 = os.path.split(temp[0])\n",
    "    \n",
    "    # get file name and parent folder from temp, temp2 respectively\n",
    "    filename = temp[1]\n",
    "    parentfile = temp2[1]\n",
    "    \n",
    "    # print (filename, parentfile)\n",
    "    # print (type(filename))\n",
    "    \n",
    "    # extract state, year, and inflow/outflow\n",
    "    # three different naming conventions in the StateMigration dataset\n",
    "    # 1) [State][Year1Year2 e.g. (0708)][in/out]\n",
    "    # 2) [State][Year2 e.g. 91][In/Out]\n",
    "    # 3) [Year1Year2 like 1)]inmig[in/out][state INITIAL e.g. AL]\n",
    "    # 4) [first 4 letters of State][Year2][in/ot]\n",
    "    # 5) s9[last digits of Year1, Year2 e.g. 56][state INITIAL][ir/or]\n",
    "    # 6) same as 4) but with extra \"r\" at the end\n",
    "    \n",
    "    # naming convention 1: used for years 2004-2009\n",
    "    name1 = [2004,2005,2006,2007,2008]\n",
    "    # naming convention 2: used for years 1990-1993\n",
    "    name2 = [1990,1991,1992]\n",
    "    # naming convention 3: used for years 2009-2011\n",
    "    name3 = [2009,2010]\n",
    "    # naming convention 4: used for years 1993-1995, 1996-2000, 2001-2004\n",
    "    name4 = [1993,1994,1996,1997, 1998, 1999, 2001,2002,2003]\n",
    "    # naming convention 5: used for years 1995-1996\n",
    "    name5 = 1995\n",
    "    # naming convention 6: used for years 2000-2001\n",
    "    name6 = 2000\n",
    "    \n",
    "    # extract inflow/outflow, year using parentfile, and state using filename\n",
    "    if parentfile[-6] == 'u':\n",
    "        io = parentfile[-7:]\n",
    "    elif parentfile[-5] == 'n':\n",
    "        io = parentfile[-6:]\n",
    "    year = int(parentfile[0:4])\n",
    "    \n",
    "    #2009to2010StateMigrationInflow\n",
    "    # print(io, year)\n",
    "    \n",
    "    if year in name1:\n",
    "        if io == 'Inflow':\n",
    "            state = filename[:-10]\n",
    "        elif io == 'Outflow':\n",
    "            state = filename[:-11]\n",
    "    elif year in name2:\n",
    "        if io == 'Inflow':\n",
    "            state = filename[:-8]\n",
    "        elif io == 'Outflow':\n",
    "            state = filename[:-9]\n",
    "    elif year in name3:\n",
    "        state = filename[-6:-4]\n",
    "    elif year in name4:\n",
    "        state = filename[:4]\n",
    "        if state == 'vrg9':\n",
    "            state = 'vrg'\n",
    "        elif state == 'vrg0':\n",
    "            state = 'vrg'\n",
    "        elif state == 'az94':\n",
    "            state = 'az'\n",
    "        elif state == 'aka9':\n",
    "            state = 'aka'\n",
    "        elif state == 'wsc9':\n",
    "            state = 'wsc'\n",
    "    elif year == name5:\n",
    "        state = filename[-8:-6]\n",
    "    elif year == name6:\n",
    "        state = filename[:4]\n",
    "        if state == 'vrg0':\n",
    "            state = 'vrg'\n",
    "        elif state == 'oeg0':\n",
    "            state = 'oeg'\n",
    "\n",
    "    si = getKey(state)\n",
    "    if si != False:\n",
    "        row_param = [si, year, io]\n",
    "    # print(row_param)\n",
    "    \n",
    "    # the total flow data in each years are located in different rows and columns.\n",
    "    # type1 - 1990, 1991: located in row 9, columns D and F\n",
    "    type1 = [1990, 1991]\n",
    "    # type2 - 1992-1994, 2004-2006: located in row 9, columns D and E\n",
    "    type2 = [1992,1993,1994,2004,2005,2006]\n",
    "    # type3 - 1995-2003, 2007-2008: located in row 10, columns D and E\n",
    "    type3 = [1995,1996,1997,1998,1999,2000,2001,2002,2003,2007,2008]\n",
    "    # type4 - 2009-2010: located in row 8, columns E and F\n",
    "    type4 = [2009,2010]\n",
    "    \n",
    "    data = pd.read_csv(csv_file)\n",
    "    # print(data)\n",
    "    if si != False:\n",
    "        if year in type1:\n",
    "            # total = data.iloc[7] #7, np.r_[3,5]\n",
    "            totaltemp = data.iat[7,0]\n",
    "            list = totaltemp.split(\",\")\n",
    "            nor = list[3]\n",
    "            noe = list[5]\n",
    "        elif year in type2:\n",
    "            # total = data.iloc[7] #7\n",
    "            totaltemp = data.iat[7,0]\n",
    "            #print(totaltemp)\n",
    "            list = totaltemp.split(\",\")\n",
    "            # print(list)\n",
    "            nor = list[3]\n",
    "            noe = list[4]\n",
    "        elif year in type3:\n",
    "            if year == 2003:\n",
    "                nor = data.iat[8,4]\n",
    "                noe = data.iat[8,5]\n",
    "            elif year == 1997:\n",
    "                nor = data.iat[8,4]\n",
    "                noe = data.iat[8,5]\n",
    "            else:\n",
    "                totaltemp = data.iat[7,0]\n",
    "                list = totaltemp.split(\",\")\n",
    "                nor = list[3]\n",
    "                noe = list[4]\n",
    "        elif year in type4:\n",
    "            total = data.iloc[6]\n",
    "            nor = data.iat[6,4]\n",
    "            noe = data.iat[6,5]\n",
    "        # print(data)    \n",
    "        # print(total)\n",
    "        # print(nor, noe)\n",
    "        # print(data)\n",
    "        \n",
    "        row_param.append(nor)\n",
    "        row_param.append(noe)\n",
    "        \n",
    "        list_param.append(row_param)\n",
    "        # print(row_param)\n",
    "        # data.shape\n",
    "        # break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "print(df)\n",
    "df.columns.values.tolist() # headers\n",
    "\n",
    "test1 = [\"FL\", 1999, 14683, 19699, 12666, 18671]\n",
    "test_df = pd.DataFrame(columns=['State', 'Year', 'NOR(I)', 'NOE(I)', 'NOR(O)', 'NOE(O)'])\n",
    "# test1 = pd.Series(test1, index=[\"FL\", 1999, 14683, 19699, 12666, 18671])\n",
    "#test_df.append([test1])\n",
    "\n",
    "# New list for append into df\n",
    "list = [\"Saurabh\", 23, \"Delhi\", \"india\"]\n",
    " \n",
    "# using loc methods\n",
    "df.loc[len(df)] = list\n",
    " \n",
    "# display\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-993e3e4c8077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m# if statement comparing state and year (indices 0 and 1 respectively)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0;31m# check if row has inflow or outflow data looking at element at index 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrow2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create a current list of what has been added to the list\n",
    "current = []\n",
    "# add parameters to df object\n",
    "for row in list_param:\n",
    "    # nested for loop to compare state and year of current list to list in question\n",
    "    if len(current) > 0: \n",
    "        for row2 in current:\n",
    "            # if statement comparing state and year (indices 0 and 1 respectively)\n",
    "            if (row2[0] == row[0]) and (row2[1] == row[1]):\n",
    "                # check if row has inflow or outflow data looking at element at index 2\n",
    "                if row2[2] == \"I\":\n",
    "                    # then add to inflow columns located at indices 2 and 3 of *row2* in current\n",
    "                    row2.insert(2,row[3])\n",
    "                    row2.insert(3,row[4])\n",
    "                    row2[2] == \"IO\"\n",
    "                elif row[2] == \"O\":\n",
    "                    # then add to outflow columns at the end of the row2 in current\n",
    "                    row2.append(row[3])\n",
    "                    row2.append(row[4])\n",
    "                    row2[2] == \"IO\"\n",
    "            else:\n",
    "                # create new list object and append to current\n",
    "                if (row[2] == \"Inflow\"):\n",
    "                    newrow = [row[0],row[1],\"I\",row[3],row[4]]\n",
    "                elif (row[2] == \"Outflow\"):\n",
    "                    newrow = [row[0],row[1],\"O\",row[3],row[4]]\n",
    "                current.append(newrow)\n",
    "    else:\n",
    "        # create new list object and append to current\n",
    "        if (row[2] == \"Inflow\"):\n",
    "            newrow = [row[0],row[1],\"I\",row[3],row[4]]\n",
    "        elif (row[2] == \"Outflow\"):\n",
    "            newrow = [row[0],row[1],\"O\",row[3],row[4]]\n",
    "        current.append(newrow) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a current list of what has been added to the list\n",
    "current = []\n",
    "# add parameters to df object\n",
    "for row in list_param:\n",
    "    # nested for loop to compare state and year of current list to list in question\n",
    "    if len(current) > 0: \n",
    "        for row2 in current:\n",
    "            # if statement comparing state and year (indices 0 and 1 respectively)\n",
    "            if (row2[0] == row[0]) and (row2[1] == row[1]):\n",
    "                # check if row has inflow or outflow data looking at element at index 2\n",
    "                if row2[2] == \"I\":\n",
    "                    # then add to inflow columns located at indices 2 and 3 of *row2* in current\n",
    "                    row2.insert(2,row[3])\n",
    "                    row2.insert(3,row[4])\n",
    "                    row2[2] == \"IO\"\n",
    "                elif row[2] == \"O\":\n",
    "                    # then add to outflow columns at the end of the row2 in current\n",
    "                    row2.append(row[3])\n",
    "                    row2.append(row[4])\n",
    "                    row2[2] == \"IO\"\n",
    "            else:\n",
    "                # create new list object and append to current\n",
    "                if (row[2] == \"Inflow\"):\n",
    "                    newrow = [row[0],row[1],\"I\",row[3],row[4]]\n",
    "                elif (row[2] == \"Outflow\"):\n",
    "                    newrow = [row[0],row[1],\"O\",row[3],row[4]]\n",
    "                current.append(newrow)\n",
    "    else:\n",
    "        # create new list object and append to current\n",
    "        if (row[2] == \"Inflow\"):\n",
    "            newrow = [row[0],row[1],\"I\",row[3],row[4]]\n",
    "        elif (row[2] == \"Outflow\"):\n",
    "            newrow = [row[0],row[1],\"O\",row[3],row[4]]\n",
    "        current.append(newrow) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out the row from csv file for NOR, NOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close output csv file\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f) and filename.endswith('.txt'):\n",
    "        read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "    read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "\n",
    "location = \"/Users/neely/Desktop/climate-displacement/code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow\"\n",
    "placement = \"./Users/neely/Desktop/climate-displacement/code/data/StateMigration\"\n",
    "\n",
    "for file in glob.glob(\"*.xls\"):\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir('../code/data/StateMigration/') if isfile(join('../code/data/StateMigration/', f))]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network -> fastai\n",
    "\n",
    "# path to dataset here (./Users/neely/Desktop/climate-displacement/code/data/StateMigration)\n",
    "path = untar_data(URLs.MNIST)\n",
    "path.ls()\n",
    "\n",
    "# data loader\n",
    "dls = ImageDataLoaders.from_folder(path, train=\"training\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

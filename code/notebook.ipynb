{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year\n",
    "State\n",
    "Households Inflow (Number of Returns)\n",
    "Households Outflow (Number of Returns)\n",
    "Individuals Inflow (Number of Exemptions)\n",
    "Individuals Outflow (Number of Exemptions)\n",
    "Chemical\n",
    "Dam/Levee Break\n",
    "Drought\n",
    "Earthquake\n",
    "Fire\n",
    "Flood\n",
    "Human Cause\n",
    "Hurricane\n",
    "Ice\n",
    "Mud/Landslide\n",
    "Other\n",
    "Snow\n",
    "Storm\n",
    "Terrorism\n",
    "Tornado\n",
    "Tsunami\n",
    "Typhoon\n",
    "Volcano\n",
    "Water\n",
    "Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFolders Setup\\n\\ncode \\n    notebook.ipynb\\n    data\\n        Disasters\\n            FEMA_dataset.csv\\n        StateMigration\\n            1990to1991StateMigration\\n                 1990to1991StateMigrationInflow\\n                     Alabama91in.xls\\n                     Alaska91in.xls\\n                     .\\n                     .\\n                     .\\n                     Wisconsin91in.xls\\n                     Wyoming91in.xls \\n                 1990to1991StateMigrationOutflow\\n                     Alabama91Out.xls\\n                     Alaska91Out.xls \\n                     .\\n                     .\\n                     .\\n                     Wisconsin91Out.xls\\n                     Wyoming91Out.xls \\n            1991to1992StateMigration\\n            1992to1993StateMigration\\n            1993to1994StateMigration\\n            1994to1995StateMigration\\n            1995to1996StateMigration\\n            1996to1997StateMigration\\n            1997to1998StateMigration\\n            1998to1999StateMigration\\n            1999to2000StateMigration\\n            2000to2001StateMigration\\n            2001to2002StateMigration\\n            2002to2003StateMigration\\n            2003to2004StateMigration\\n            2004to2005StateMigration\\n            2005to2006StateMigration\\n            2006to2007StateMigration\\n            2007to2008StateMigration\\n            2008to2009StateMigration\\n            2009to2010StateMigration\\n            2010to2011StateMigration \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Folders Setup\n",
    "\n",
    "code \n",
    "    notebook.ipynb\n",
    "    data\n",
    "        Disasters\n",
    "            FEMA_dataset.csv\n",
    "        StateMigration\n",
    "            1990to1991StateMigration\n",
    "                 1990to1991StateMigrationInflow\n",
    "                     Alabama91in.xls\n",
    "                     Alaska91in.xls\n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91in.xls\n",
    "                     Wyoming91in.xls \n",
    "                 1990to1991StateMigrationOutflow\n",
    "                     Alabama91Out.xls\n",
    "                     Alaska91Out.xls \n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91Out.xls\n",
    "                     Wyoming91Out.xls \n",
    "            1991to1992StateMigration\n",
    "            1992to1993StateMigration\n",
    "            1993to1994StateMigration\n",
    "            1994to1995StateMigration\n",
    "            1995to1996StateMigration\n",
    "            1996to1997StateMigration\n",
    "            1997to1998StateMigration\n",
    "            1998to1999StateMigration\n",
    "            1999to2000StateMigration\n",
    "            2000to2001StateMigration\n",
    "            2001to2002StateMigration\n",
    "            2002to2003StateMigration\n",
    "            2003to2004StateMigration\n",
    "            2004to2005StateMigration\n",
    "            2005to2006StateMigration\n",
    "            2006to2007StateMigration\n",
    "            2007to2008StateMigration\n",
    "            2008to2009StateMigration\n",
    "            2009to2010StateMigration\n",
    "            2010to2011StateMigration \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPseudo Code:\\n\\nFEMA Dataset Pre-Processing \\n1. Create new FEMA_dataset with columns \"Year\",  \"State\" and \"Disaster Type\"\\n    - contains Year, State, Disaster Type\\n2. Name file \"State_Disasters_by_Year\"\\n\\nStateMigration Data Pre-Processing\\n1. Convert all datasets in StateMigration from .xls into .csv files\\n2. Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\" - assign I if from inflow and O if from outflow - from every state file.\\n3. Extract \"State\" and \"Year\" from every file\\n4. Create file with \"State\" (from file name), \"Year\" (from file name), \"Number_of_Returns_I\", \"Number_of_Exemptions_I\", \"Number_of_Returns_I\" and \"Number_of_Exemptions_O\"\\n5. Name file \"State Migration by Year\"\\n\\nMerge Datasets\\n1. Merge datasets on common attributes \"Year\" and \"State\"\\n2. Name dataset \"State_Migration_and_Disasters_by_Year\"\\n\\nQuestion\\n- Where is 2003to2004 MigrationOutflow data?\\n\\n\\nread_file = pd.read_excel (\"Test.xlsx\")\\n \\n# Write the dataframe object\\n# into csv file\\nread_file.to_csv (\"Test.csv\",\\n                  index = None,\\n                  header=True)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pseudo Code:\n",
    "\n",
    "FEMA Dataset Pre-Processing \n",
    "1. Create new FEMA_dataset with columns \"Year\",  \"State\" and \"Disaster Type\"\n",
    "    - contains Year, State, Disaster Type\n",
    "2. Name file \"State_Disasters_by_Year\"\n",
    "\n",
    "StateMigration Data Pre-Processing\n",
    "1. Convert all datasets in StateMigration from .xls into .csv files\n",
    "2. Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\" - assign I if from inflow and O if from outflow - from every state file.\n",
    "3. Extract \"State\" and \"Year\" from every file\n",
    "4. Create file with \"State\" (from file name), \"Year\" (from file name), \"Number_of_Returns_I\", \"Number_of_Exemptions_I\", \"Number_of_Returns_I\" and \"Number_of_Exemptions_O\"\n",
    "5. Name file \"State Migration by Year\"\n",
    "\n",
    "Merge Datasets\n",
    "1. Merge datasets on common attributes \"Year\" and \"State\"\n",
    "2. Name dataset \"State_Migration_and_Disasters_by_Year\"\n",
    "\n",
    "Question\n",
    "- Where is 2003to2004 MigrationOutflow data?\n",
    "\n",
    "\n",
    "read_file = pd.read_excel (\"Test.xlsx\")\n",
    " \n",
    "# Write the dataframe object\n",
    "# into csv file\n",
    "read_file.to_csv (\"Test.csv\",\n",
    "                  index = None,\n",
    "                  header=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      State Disaster Type  Start Year  End Year\n",
      "0        GA       Tornado        1953    1953.0\n",
      "1        TX       Tornado        1953    1953.0\n",
      "2        LA         Flood        1953    1953.0\n",
      "3        MI       Tornado        1953    1953.0\n",
      "4        MT         Flood        1953    1953.0\n",
      "...     ...           ...         ...       ...\n",
      "46180    CA         Storm        2017    2017.0\n",
      "46181    CA         Storm        2017    2017.0\n",
      "46182    CA         Storm        2017    2017.0\n",
      "46183    CA         Storm        2017    2017.0\n",
      "46184    CA         Storm        2017    2017.0\n",
      "\n",
      "[46185 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "FEMA Dataset Pre-Processing\n",
    "\n",
    "Creates State_Disasters_by_Year.csv with:\n",
    "- State\n",
    "- Disaster Type\n",
    "- Start Year\n",
    "- End Year\n",
    "\"\"\"\n",
    "\n",
    "# copy original FEMA dataset to new file\n",
    "original = r'../code/data/Disasters/FEMA_dataset.csv'\n",
    "new = r'../code/data/Disasters/State_Disasters_by_Year.csv'\n",
    "shutil.copyfile(original, new)\n",
    "\n",
    "# read csv file\n",
    "data = pd.read_csv('../code/data/Disasters/State_Disasters_by_Year.csv')\n",
    "\n",
    "# delete irrelevant rows\n",
    "data.pop('Declaration Number')\n",
    "data.pop('Declaration Type')\n",
    "data.pop('Declaration Date')\n",
    "data.pop('County')\n",
    "data.pop('Disaster Title')\n",
    "data.pop('Close Date')\n",
    "data.pop('Individual Assistance Program')\n",
    "data.pop('Public Assistance Program')\n",
    "data.pop('Hazard Mitigation Program')\n",
    "data.pop('Individuals & Households Program')\n",
    "\n",
    "# extract years\n",
    "data['Start Year'] = pd.DatetimeIndex(data['Start Date']).year\n",
    "data['End Year'] = pd.DatetimeIndex(data['End Date']).year\n",
    "\n",
    "# delete start and end dates\n",
    "data.pop('Start Date')\n",
    "data.pop('End Date')\n",
    "\n",
    "print(data)\n",
    "\n",
    "# save changes csv\n",
    "data.to_csv('../code/data/Disasters/State_Disasters_by_Year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7bb34d1c49e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# replace xls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxls_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxls_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mread_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxls_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mread_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxls_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#read_xls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;31m# default to avoid a ValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Convert all files in StateMigration folder from .xls to .csv\n",
    "\n",
    "# create list of xls files\n",
    "xls_list = glob.glob(\"../code/data/StateMigration/*/*/*.xls\")\n",
    "\n",
    "# replace xls \n",
    "for xls_file in xls_list:\n",
    "    read_file = pd.read_table(xls_file)\n",
    "    read_file.to_csv(xls_file[0:-3]+'csv', index = None, header=True)\n",
    "    #read_xls\n",
    "    #DataFrame\n",
    "    #print to_csv\n",
    "    \n",
    "    os.remove(xls_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f) and filename.endswith('.txt'):\n",
    "        read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "    read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "\n",
    "location = \"/Users/neely/Desktop/climate-displacement/code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow\"\n",
    "placement = \"./Users/neely/Desktop/climate-displacement/code/data/StateMigration\"\n",
    "\n",
    "for file in glob.glob(\"*.xls\"):\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir('../code/data/StateMigration/') if isfile(join('../code/data/StateMigration/', f))]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

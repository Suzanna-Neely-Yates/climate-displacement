{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disaster Types\n",
    "- Year\n",
    "- State\n",
    "- Households Inflow (Number of Returns)\n",
    "- Households Outflow (Number of Returns)\n",
    "- Individuals Inflow (Number of Exemptions)\n",
    "- Individuals Outflow (Number of Exemptions)\n",
    "- Chemical\n",
    "- Dam/Levee Break\n",
    "- Drought\n",
    "- Earthquake\n",
    "- Fire\n",
    "- Flood\n",
    "- Human Cause\n",
    "- Hurricane\n",
    "- Ice\n",
    "- Mud/Landslide\n",
    "- Other\n",
    "- Snow\n",
    "- Storm\n",
    "- Terrorism\n",
    "- Tornado\n",
    "- Tsunami\n",
    "- Typhoon\n",
    "- Volcano\n",
    "- Water\n",
    "- Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFolders Setup\\n\\ncode \\n    notebook.ipynb\\n    data\\n        Disasters\\n            FEMA_dataset.csv\\n        StateMigration\\n            1990to1991StateMigration\\n                 1990to1991StateMigrationInflow\\n                     Alabama91in.xls\\n                     Alaska91in.xls\\n                     .\\n                     .\\n                     .\\n                     Wisconsin91in.xls\\n                     Wyoming91in.xls \\n                 1990to1991StateMigrationOutflow\\n                     Alabama91Out.xls\\n                     Alaska91Out.xls \\n                     .\\n                     .\\n                     .\\n                     Wisconsin91Out.xls\\n                     Wyoming91Out.xls \\n            .\\n            .\\n            .\\n            2008to2009StateMigration\\n            2009to2010StateMigration\\n            2010to2011StateMigration \\n'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Folders Setup\n",
    "\n",
    "code \n",
    "    notebook.ipynb\n",
    "    data\n",
    "        Disasters\n",
    "            FEMA_dataset.csv\n",
    "        StateMigration\n",
    "            1990to1991StateMigration\n",
    "                 1990to1991StateMigrationInflow\n",
    "                     Alabama91in.xls\n",
    "                     Alaska91in.xls\n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91in.xls\n",
    "                     Wyoming91in.xls \n",
    "                 1990to1991StateMigrationOutflow\n",
    "                     Alabama91Out.xls\n",
    "                     Alaska91Out.xls \n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91Out.xls\n",
    "                     Wyoming91Out.xls \n",
    "            .\n",
    "            .\n",
    "            .\n",
    "            2008to2009StateMigration\n",
    "            2009to2010StateMigration\n",
    "            2010to2011StateMigration \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode:\n",
    "\n",
    "### FEMA Dataset Pre-processing (Neely)\n",
    "1. Create new FEMA_dataset with columns \n",
    "    - contains Year, State, Disaster Type\n",
    "2. Name file \"State_Disasters_by_Year\"\n",
    "\n",
    "### StateMigration Data Pre-Processing (Ben)\n",
    "1. Convert all datasets in StateMigration from .xls into .csv files\n",
    "2. Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\" - assign I if from inflow and O if from outflow - from every state file.\n",
    "3. Extract \"State\" and \"Year\" from every file\n",
    "4. Create file with \"State\" (from file name), \"Year\" (from file name), \"Number_of_Returns_I\", \"Number_of_Exemptions_I\", \"Number_of_Returns_I\" and \"Number_of_Exemptions_O\"\n",
    "5. Name file \"State Migration by Year\"\n",
    "\n",
    "### Merge Datasets (Both)\n",
    "1. Merge datasets on common attributes \"Year\" and \"State\"\n",
    "2. Name dataset \"State_Migration_and_Disasters_by_Year\"\n",
    "\n",
    "#### Train and Testing\n",
    "1. Create training and testing datasets\n",
    "    Questions: How should we split training and testing data?\n",
    "2. Create Neural Network models\n",
    "    Input: Year, State, Disaster Type\n",
    "    Output: Migration Inflow (Household/Individual), Migration Outflow (Household/Individual)\n",
    "3. Put training and testing through the Neural Network models.\n",
    "4. Evaluate which models are the most effective.\n",
    "---\n",
    "Data Augmentation\n",
    "Synthetic Data\n",
    "Use svm or decision tree -- skleant -- and compare against a neural network\n",
    "could use all data for training and all data for validation - not this is a faulty practice in \n",
    "20% distribution of state \n",
    "\n",
    "\n",
    "```\n",
    "read_file = pd.read_excel (\"Test.xlsx\")\n",
    " \n",
    "# Write the dataframe object\n",
    "# into csv file\n",
    "read_file.to_csv (\"Test.csv\",\n",
    "                  index = None,\n",
    "                  header=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding imports\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "#import xlrd\n",
    "import csv\n",
    "import numpy as np\n",
    "#from fastai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEMA Dataset Pre-processing\n",
    "\n",
    "Creates State_Disasters_by_Year.csv with:\n",
    "- State\n",
    "- Disaster Type\n",
    "- Start Year\n",
    "- End Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      State Disaster Type  Start Year  End Year\n",
      "0        TX        Winter        1989    1989.0\n",
      "1        TX        Winter        1989    1989.0\n",
      "2        TX        Winter        1989    1989.0\n",
      "3        TX        Winter        1989    1989.0\n",
      "4        TX        Winter        1989    1989.0\n",
      "...     ...           ...         ...       ...\n",
      "36785    CA         Storm        2017    2017.0\n",
      "36786    CA         Storm        2017    2017.0\n",
      "36787    CA         Storm        2017    2017.0\n",
      "36788    CA         Storm        2017    2017.0\n",
      "36789    CA         Storm        2017    2017.0\n",
      "\n",
      "[36790 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# FEMA Dataset Preprocessing\n",
    "\n",
    "# copy original FEMA dataset to new file\n",
    "original = r'../code/data/Disasters/FEMA_dataset.csv'\n",
    "new = r'../code/data/Disasters/State_Disasters_by_Year.csv'\n",
    "shutil.copyfile(original, new)\n",
    "\n",
    "# read csv file\n",
    "data = pd.read_csv('../code/data/Disasters/State_Disasters_by_Year.csv')\n",
    "\n",
    "# delete irrelevant rows\n",
    "data.pop('Declaration Number')\n",
    "data.pop('Declaration Type')\n",
    "data.pop('Declaration Date')\n",
    "data.pop('County')\n",
    "data.pop('Disaster Title')\n",
    "data.pop('Close Date')\n",
    "data.pop('Individual Assistance Program')\n",
    "data.pop('Public Assistance Program')\n",
    "data.pop('Hazard Mitigation Program')\n",
    "data.pop('Individuals & Households Program')\n",
    "\n",
    "# extract years\n",
    "data['Start Year'] = pd.DatetimeIndex(data['Start Date']).year\n",
    "data['End Year'] = pd.DatetimeIndex(data['End Date']).year\n",
    "\n",
    "# delete start and end dates\n",
    "data.pop('Start Date')\n",
    "data.pop('End Date')\n",
    "\n",
    "print(data)\n",
    "\n",
    "# save changes csv\n",
    "data.to_csv('../code/data/Disasters/State_Disasters_by_Year.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting State Migration data from .xls to .csv\n",
    "\n",
    "Converting all datasets in StateMigration from .xls to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all files in StateMigration folder from .xls to .csv\n",
    "\n",
    "# create list of xls files\n",
    "xls_list = glob.glob(\"/Users/ben/Desktop/climate-displacement/code/data/StateMigration/*/*/*.xls\")\n",
    "\n",
    "# replace xls \n",
    "for xls_file in xls_list:\n",
    "    \n",
    "    wb = xlrd.open_workbook(xls_file)\n",
    "    sh = wb.sheet_by_index(0)\n",
    "    csv_file = open(xls_file[0:-3]+'csv', \"w\")\n",
    "    wr = csv.writer(csv_file, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    for rownum in range(sh.nrows):\n",
    "        wr.writerow(sh.row_values(rownum))\n",
    "        \n",
    "    csv_file.close()\n",
    "    \n",
    "    # remove .xls files\n",
    "    os.remove(xls_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More data wrangling - StateMigration dataset\n",
    "- Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\"\n",
    "- Assign \"I\" if from inflow and \"O\" if from outflow - for every state file\n",
    "- Extract \"State\" and \"Year\" from every file\n",
    "- Create file with \"State\" (from file name), \"Year\" (from file name), \"Number of Returns_I\", \"Number of Exemptions I\", \"Number of Returns O\" and \"Number of Exemptions O\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "1. create output path in repository for the merged StateMigration dataset\n",
    "\n",
    "2. read each csv file in the StateMigration folder, and for each file:\n",
    "\n",
    "    a. check the csv files to make sure they are the intended data\n",
    "    \n",
    "    b. remove the original csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new file location for merged StateMigration dataset\n",
    "output_path = r'../code/data/StateMigration/State_Migrations_by_Year.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output file at output_path\n",
    "output = open(output_path, \"w\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty DataFrame object\n",
    "df = pd.DataFrame()\n",
    "df.insert(0,'State', '')\n",
    "df.insert(1,'Year', '')\n",
    "df.insert(2,'NOR(I)', '')\n",
    "df.insert(3,'NOE(I)', '')\n",
    "df.insert(4,'NOR(O)', '')\n",
    "df.insert(5,'NOE(O)', '')\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WI\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of \"state initial keys\" with multiple \"values\"\n",
    "# run each segment through dictionary, and convert into state initial\n",
    "stateDict = {\n",
    "    \"AL\":['Alabama', 'al', 'AL', 'alab', 'Alab'],\n",
    "    \"AK\":['Alaska', 'ak', 'AK', 'alas', 'Alas'],\n",
    "    \"AZ\":['Arizona', 'az', 'AZ', 'ariz', 'Ariz'],\n",
    "    \"AR\":['Arkansas', 'ar', 'AR', 'arka', 'Arka', 'aka'],\n",
    "    \"CA\":['California', 'ca', 'CA', 'cali', 'Cali'],\n",
    "    \"CO\":['Colorado', 'co', 'CO', 'colo', 'Colo'],\n",
    "    \"CT\":['Connecticut', 'ct', 'CT', 'conn', 'Conn'],\n",
    "    \"DE\":['Delaware', 'de', 'DE', 'dela', 'Dela'],\n",
    "    \"DC\":['DistrictofColumbia', 'Districtofcolumbia', 'District of Columbia', 'dc', 'DC', 'dist', 'Dist', 'DiCo', 'dico'],\n",
    "    \"FL\":['Florida', 'fl', 'FL', 'flor', 'Flor'],\n",
    "    \"GA\":['Georgia', 'ga', 'GA', 'geor', 'Geor'],\n",
    "    \"HI\":['Hawaii', 'hi', 'HI', 'hawa', 'Hawa'],\n",
    "    \"ID\":['Idaho', 'id', 'ID', 'idah', 'Idah'],\n",
    "    \"IL\":['Illinois', 'il', 'IL', 'illi', 'Illi'],\n",
    "    \"IN\":['Indiana', 'in', 'IN', 'indi', 'Indi'],\n",
    "    \"IA\":['Iowa', 'ia', 'IA', 'iowa'],\n",
    "    \"KS\":['Kansas', 'ks', 'KS', 'kans', 'Kans'],\n",
    "    \"KY\":['Kentucky', 'ky', 'KY', 'kent', 'Kent'],\n",
    "    \"LA\":['Louisiana', 'la', 'LA', 'loui', 'Loui'],\n",
    "    \"MA\":['Massachusetts', 'ma', 'MA', 'mass', 'Mass'],\n",
    "    \"MD\":['Maryland', 'md', 'MD', 'mary', 'Mary'],\n",
    "    \"ME\":['Maine', 'me', 'ME', 'main', 'Main'],\n",
    "    \"MI\":['Michigan', 'mi', 'MI', 'mich', 'Mich'],\n",
    "    \"MN\":['Minnesota', 'mn', 'MN', 'minn', 'Minn'],\n",
    "    \"MO\":['Missouri', 'mo', 'MO', 'Miso', 'miso'],\n",
    "    \"MS\":['Mississippi', 'ms', 'MS', 'Misi', 'misi', 'miss', 'Miss'],\n",
    "    \"MT\":['Montana', 'mt', 'MT', 'mont', 'Mont'],\n",
    "    \"NC\":['North Carolina', 'NorthCarolina', 'nc', 'NC', 'NoCa', 'noca', 'ncar', 'Northcarolina'],\n",
    "    \"ND\":['North Dakota', 'NorthDakota', 'nd', 'ND', 'NoDa', 'noda', 'ndak', 'Northdakota'],\n",
    "    \"NE\":['Nebraska', 'ne', 'NE', 'Nebr', 'nrbt', 'nebr'],\n",
    "    \"NH\":['New Hampshire', 'NewHampshire', 'nh', 'NH', 'NeHa', 'neha', 'newh'],\n",
    "    \"NJ\":['New Jersey', 'NewJersey', 'nj', 'NJ', 'NeJe', 'neje', 'newj', 'Newjersey'],\n",
    "    \"NM\":['New Mexico', 'NewMexico', 'nm', 'NM', 'NeMe', 'neme', 'newm', 'Newmexico'],\n",
    "    \"NV\":['Nevada', 'nv', 'NV', 'Neva', 'neva'],\n",
    "    \"NY\":['New York', 'NewYork', 'ny', 'NY', 'newy', 'NeYo', 'neyo', 'newY','Newyork'],\n",
    "    \"OH\":['Ohio', 'oh', 'OH', 'ohio', 'nhio'],\n",
    "    \"OK\":['Oklahoma', 'ok', 'OK', 'okla', 'Okla'],\n",
    "    \"OR\":['Oregon', 'or', 'OR', 'oreg', 'Oreg', 'oeg'],\n",
    "    \"PA\":['Pennsylvania', 'pa', 'PA', 'penn', 'Penn'],\n",
    "    \"RI\":['Rhode Island', 'RhodeIsland', 'ri', 'RI', 'Rhls', 'rhod', 'Rhod', 'RhIs'],\n",
    "    \"SC\":['South Carolina', 'SouthCarolina', 'sc', 'SC', 'SoCa', 'soca', 'scar', 'Southcarolina'],\n",
    "    \"SD\":['South Dakota', 'SouthDakota', 'sd', 'SD', 'SoDa', 'soda', 'sdak', 'Southdakota'],\n",
    "    \"TN\":['Tennessee', 'tn', 'TN', 'Tenn', 'tenn'],\n",
    "    \"TX\":['Texas', 'tx', 'TX', 'texa', 'Texa'],\n",
    "    \"UT\":['Utah', 'ut', 'UT', 'utah'],\n",
    "    \"VA\":['Virginia', 'va', 'VA', 'virg', 'Virg', 'vrg'],\n",
    "    \"VT\":['Vermont', 'vt', 'VT', 'verm', 'Verm'],\n",
    "    \"WA\":['Washington', 'wa', 'WA', 'wash', 'Wash'],\n",
    "    \"WI\":['Wisconsin', 'wi', 'WI', 'wisc', 'Wisc', 'wiso', 'wsc'],\n",
    "    \"WV\":['West Virginia', 'WestVirginia', 'wv', 'WV', 'west', 'wevi', 'wvir', 'Westvirginia'],\n",
    "    \"WY\":['Wyoming', 'wy', 'WY', 'wyom', 'Wyom']    \n",
    "}\n",
    "\n",
    "def getKey(val):\n",
    "    for key, valueList in stateDict.items():\n",
    "         for value in valueList:\n",
    "            if val == value:\n",
    "                 return key\n",
    "    \n",
    "    return False\n",
    "\n",
    "print(getKey('Wisconsin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read csv file\n",
    "# csv_list = glob.glob(\"/Users/ben/Desktop/climate-displacement/code/data/StateMigration/*/*/*.csv\")\n",
    "csv_list = glob.glob(\"../code/data/StateMigration/*/*/*.csv\")\n",
    "\n",
    "# create list of row_params\n",
    "list_param = []\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    # os.path.split returns a list of (head, tail) where head is the parent directories \n",
    "    # and tail is the filename and extension\n",
    "    temp = os.path.split(csv_file)\n",
    "    temp2 = os.path.split(temp[0])\n",
    "    \n",
    "    # get file name and parent folder from temp, temp2 respectively\n",
    "    filename = temp[1]\n",
    "    parentfile = temp2[1]\n",
    "    \n",
    "    # print (filename, parentfile)\n",
    "    # print (type(filename))\n",
    "    \n",
    "    # extract state, year, and inflow/outflow\n",
    "    # three different naming conventions in the StateMigration dataset\n",
    "    # 1) [State][Year1Year2 e.g. (0708)][in/out]\n",
    "    # 2) [State][Year2 e.g. 91][In/Out]\n",
    "    # 3) [Year1Year2 like 1)]inmig[in/out][state INITIAL e.g. AL]\n",
    "    # 4) [first 4 letters of State][Year2][in/ot]\n",
    "    # 5) s9[last digits of Year1, Year2 e.g. 56][state INITIAL][ir/or]\n",
    "    # 6) same as 4) but with extra \"r\" at the end\n",
    "    \n",
    "    # naming convention 1: used for years 2004-2009\n",
    "    name1 = [2004,2005,2006,2007,2008]\n",
    "    # naming convention 2: used for years 1990-1993\n",
    "    name2 = [1990,1991,1992]\n",
    "    # naming convention 3: used for years 2009-2011\n",
    "    name3 = [2009,2010]\n",
    "    # naming convention 4: used for years 1993-1995, 1996-2000, 2001-2004\n",
    "    name4 = [1993,1994,1996,1997, 1998, 1999, 2001,2002,2003]\n",
    "    # naming convention 5: used for years 1995-1996\n",
    "    name5 = 1995\n",
    "    # naming convention 6: used for years 2000-2001\n",
    "    name6 = 2000\n",
    "    \n",
    "    # extract inflow/outflow, year using parentfile, and state using filename\n",
    "    if parentfile[-6] == 'u':\n",
    "        io = parentfile[-7:]\n",
    "    elif parentfile[-5] == 'n':\n",
    "        io = parentfile[-6:]\n",
    "    year = int(parentfile[0:4])\n",
    "    \n",
    "    #2009to2010StateMigrationInflow\n",
    "    # print(io, year)\n",
    "    \n",
    "    if year in name1:\n",
    "        if io == 'Inflow':\n",
    "            state = filename[:-10]\n",
    "        elif io == 'Outflow':\n",
    "            state = filename[:-11]\n",
    "    elif year in name2:\n",
    "        if io == 'Inflow':\n",
    "            state = filename[:-8]\n",
    "        elif io == 'Outflow':\n",
    "            state = filename[:-9]\n",
    "    elif year in name3:\n",
    "        state = filename[-6:-4]\n",
    "    elif year in name4:\n",
    "        state = filename[:4]\n",
    "        if state == 'vrg9':\n",
    "            state = 'vrg'\n",
    "        elif state == 'vrg0':\n",
    "            state = 'vrg'\n",
    "        elif state == 'az94':\n",
    "            state = 'az'\n",
    "        elif state == 'aka9':\n",
    "            state = 'aka'\n",
    "        elif state == 'wsc9':\n",
    "            state = 'wsc'\n",
    "    elif year == name5:\n",
    "        state = filename[-8:-6]\n",
    "    elif year == name6:\n",
    "        state = filename[:4]\n",
    "        if state == 'vrg0':\n",
    "            state = 'vrg'\n",
    "        elif state == 'oeg0':\n",
    "            state = 'oeg'\n",
    "\n",
    "    si = getKey(state)\n",
    "    if si != False:\n",
    "        row_param = [si, year, io]\n",
    "    # print(row_param)\n",
    "    \n",
    "    # the total flow data in each years are located in different rows and columns.\n",
    "    # type1 - 1990, 1991: located in row 9, columns D and F\n",
    "    type1 = [1990, 1991]\n",
    "    # type2 - 1992-1994, 2004-2006: located in row 9, columns D and E\n",
    "    type2 = [1992,1993,1994,2004,2005,2006]\n",
    "    # type3 - 1995-2003, 2007-2008: located in row 10, columns D and E\n",
    "    type3 = [1995,1996,1997,1998,1999,2000,2001,2002,2003,2007,2008]\n",
    "    # type4 - 2009-2010: located in row 8, columns E and F\n",
    "    type4 = [2009,2010]\n",
    "    \n",
    "    data = pd.read_csv(csv_file)\n",
    "    # print(data)\n",
    "    if si != False:\n",
    "        if year in type1:\n",
    "            # total = data.iloc[7] #7, np.r_[3,5]\n",
    "            totaltemp = data.iat[7,0]\n",
    "            list = totaltemp.split(\",\")\n",
    "            if ((si == \"AL\") and (io == \"Outflow\")):\n",
    "                nor = list[4]\n",
    "                noe = list[6]\n",
    "            else:\n",
    "                nor = list[3]\n",
    "                noe = list[5]\n",
    "        elif year in type2:\n",
    "            # total = data.iloc[7] #7\n",
    "            totaltemp = data.iat[7,0]\n",
    "            #print(totaltemp)\n",
    "            list = totaltemp.split(\",\")\n",
    "            # print(list)\n",
    "            nor = list[3]\n",
    "            noe = list[4]\n",
    "        elif year in type3:\n",
    "            if year == 2003:\n",
    "                nor = data.iat[8,4]\n",
    "                noe = data.iat[8,5]\n",
    "            elif year == 1997:\n",
    "                nor = data.iat[8,4]\n",
    "                noe = data.iat[8,5]\n",
    "            elif year == 2002:\n",
    "                totaltemp = data.iat[8,0]\n",
    "                list = totaltemp.split(\",\")\n",
    "                nor = list[3]\n",
    "                noe = list[4]\n",
    "            else:\n",
    "                totaltemp = data.iat[7,0]\n",
    "                list = totaltemp.split(\",\")\n",
    "                nor = list[3]\n",
    "                noe = list[4]\n",
    "        elif year in type4:\n",
    "            total = data.iloc[6]\n",
    "            nor = data.iat[6,4]\n",
    "            noe = data.iat[6,5]\n",
    "        # print(data)    \n",
    "        # print(total)\n",
    "        # print(nor, noe)\n",
    "        # print(data)\n",
    "        \n",
    "        row_param.append(nor)\n",
    "        row_param.append(noe)\n",
    "        \n",
    "        list_param.append(row_param)\n",
    "        # print(row_param)\n",
    "        # data.shape\n",
    "        # break\n",
    "#print(list_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = []\n",
    "# print(list_param)\n",
    "for row in list_param:\n",
    "    isIOMatch = False\n",
    "\n",
    "    for row2 in current:\n",
    "        if ((row[0] == row2[0]) and (row[1] == row2[1])):\n",
    "            \n",
    "            if row[2] == \"Inflow\": # you have inflow data to add, outflow already exists\n",
    "                row2.insert(3, row[3]) #0-si, 1-year, 2-io, 3-nor, 4-noe\n",
    "                row2.insert(4, row[4])\n",
    "                row2.pop(2)\n",
    "                isIOMatch = True\n",
    "            elif row[2] == \"Outflow\": # you have outflow data to add, inflow already exists\n",
    "                row2.append(row[3])\n",
    "                row2.append(row[4])\n",
    "                row2.pop(2)\n",
    "                isIOMatch = True\n",
    "    if isIOMatch == False:\n",
    "        if row[2] == \"Inflow\": # adding inflow data\n",
    "            newrow = [row[0],row[1],\"Inflow\",row[3],row[4]]\n",
    "        elif (row[2] == \"Outflow\"): # adding outflow data\n",
    "            newrow = [row[0],row[1],\"Outflow\",row[3],row[4]]\n",
    "        current.append(newrow)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     State  Year    NOR(I)    NOE(I)    NOR(O)    NOE(O)\n",
      "0       MN  2010   41038.0   73463.0   45173.0   77893.0\n",
      "1       AZ  2010   83528.0  158038.0   80278.0  162164.0\n",
      "2       AL  2010   42880.0   89794.0   43563.0   88001.0\n",
      "3       MO  2010   55637.0  106395.0   60740.0  115708.0\n",
      "4       NC  2010  114845.0  226709.0  101963.0  201396.0\n",
      "...    ...   ...       ...       ...       ...       ...\n",
      "1066    WA  2009   77278.0  147288.0   72538.0  136914.0\n",
      "1067    TX  2009  204851.0  421684.0  163893.0  328137.0\n",
      "1068    OR  2009   44892.0   78364.0   41180.0   74673.0\n",
      "1069    NV  2009   44554.0   82221.0   47379.0   91828.0\n",
      "1070    TN  2009   70156.0  139051.0   64116.0  127939.0\n",
      "\n",
      "[1071 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(current)\n",
    "data = pd.DataFrame(current, columns=['State', 'Year', 'NOR(I)', 'NOE(I)', 'NOR(O)', 'NOE(O)'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert df into csv?\n",
    "stmg_csv = data.to_csv(output_path)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom os.path import isfile, join\\nonlyfiles = [f for f in listdir('../code/data/StateMigration/') if isfile(join('../code/data/StateMigration/', f))]\\n\""
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f) and filename.endswith('.txt'):\n",
    "        read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "    read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "\n",
    "location = \"/Users/neely/Desktop/climate-displacement/code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow\"\n",
    "placement = \"./Users/neely/Desktop/climate-displacement/code/data/StateMigration\"\n",
    "\n",
    "for file in glob.glob(\"*.xls\"):\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir('../code/data/StateMigration/') if isfile(join('../code/data/StateMigration/', f))]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     State  Year    NOR(I)    NOE(I)    NOR(O)    NOE(O)  Drought  Earthquake  \\\n",
      "0       MN  2010   41038.0   73463.0   45173.0   77893.0        0           0   \n",
      "1       AZ  2010   83528.0  158038.0   80278.0  162164.0        0           0   \n",
      "2       AL  2010   42880.0   89794.0   43563.0   88001.0        0           0   \n",
      "3       MO  2010   55637.0  106395.0   60740.0  115708.0        0           0   \n",
      "4       NC  2010  114845.0  226709.0  101963.0  201396.0        0           0   \n",
      "...    ...   ...       ...       ...       ...       ...      ...         ...   \n",
      "1066    WA  2009   77278.0  147288.0   72538.0  136914.0        0           0   \n",
      "1067    TX  2009  204851.0  421684.0  163893.0  328137.0        0           0   \n",
      "1068    OR  2009   44892.0   78364.0   41180.0   74673.0        0           0   \n",
      "1069    NV  2009   44554.0   82221.0   47379.0   91828.0        0           0   \n",
      "1070    TN  2009   70156.0  139051.0   64116.0  127939.0        0           0   \n",
      "\n",
      "      Fire  Flood  ...  Snow  Storm  Terrorism  Tornado  Tsunami  Typhoon  \\\n",
      "0        0      0  ...     0      0          0        0        0        0   \n",
      "1        0      0  ...     0      0          0        0        0        0   \n",
      "2        0      0  ...     0      0          0        0        0        0   \n",
      "3        0      0  ...     0      0          0        0        0        0   \n",
      "4        0      0  ...     0      0          0        0        0        0   \n",
      "...    ...    ...  ...   ...    ...        ...      ...      ...      ...   \n",
      "1066     0      0  ...     0      0          0        0        0        0   \n",
      "1067     0      0  ...     0      0          0        0        0        0   \n",
      "1068     0      0  ...     0      0          0        0        0        0   \n",
      "1069     0      0  ...     0      0          0        0        0        0   \n",
      "1070     0      0  ...     0      0          0        0        0        0   \n",
      "\n",
      "      Volcano  Water  Winter  Total disasters  \n",
      "0           0      0       0                0  \n",
      "1           0      0       0                0  \n",
      "2           0      0       0                0  \n",
      "3           0      0       0                0  \n",
      "4           0      0       0                0  \n",
      "...       ...    ...     ...              ...  \n",
      "1066        0      0       0                0  \n",
      "1067        0      0       0                0  \n",
      "1068        0      0       0                0  \n",
      "1069        0      0       0                0  \n",
      "1070        0      0       0                0  \n",
      "\n",
      "[1071 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Create final dataset\n",
    "\n",
    "1. Copy State_Disaster_by_Year\n",
    "2. Add column for each natural disaster\n",
    "3. Join over common year and state matches\n",
    "4. Increase number of natural disasters if natural disaster of same type already exists in a year\n",
    "\"\"\"\n",
    "\n",
    "# FEMA Dataset Preprocessing\n",
    "\n",
    "# copy data from State_Migration_by_Year.csv\n",
    "original = r'../code/data/StateMigration/State_Migrations_by_Year.csv'\n",
    "new = r'../code/data/Total/Neural_Network_Input.csv'\n",
    "shutil.copyfile(original, new)\n",
    "\n",
    "# read csv file\n",
    "data = pd.read_csv(new)\n",
    "\n",
    "# delete irrelevant rows\n",
    "data.pop('Unnamed: 0')\n",
    "\n",
    "# create climate disaster columns\n",
    "data[\"Drought\"] = 0\n",
    "data[\"Earthquake\"] = 0\n",
    "data[\"Fire\"] = 0\n",
    "data[\"Flood\"] = 0\n",
    "data[\"Human Cause\"] = 0\n",
    "data[\"Hurricane\"] = 0\n",
    "data[\"Ice\"] = 0\n",
    "data[\"Mud/Landslide\"] = 0\n",
    "data[\"Other\"] = 0\n",
    "data[\"Snow\"] = 0\n",
    "data[\"Storm\"] = 0\n",
    "data[\"Terrorism\"] = 0\n",
    "data[\"Tornado\"] = 0\n",
    "data[\"Tsunami\"] = 0\n",
    "data[\"Typhoon\"] = 0\n",
    "data[\"Volcano\"] = 0\n",
    "data[\"Water\"] = 0\n",
    "data [\"Winter\"] = 0\n",
    "data[\"Total disasters\"] = 0\n",
    "\n",
    "data.to_csv('../code/data/Total/Neural_Network_Input.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "else\n",
      "else\n",
      "else\n",
      "     State  Year    NOR(I)    NOE(I)    NOR(O)    NOE(O)  Drought  Earthquake  \\\n",
      "0       MN  2010   41038.0   73463.0   45173.0   77893.0        0           0   \n",
      "1       AZ  2010   83528.0  158038.0   80278.0  162164.0        0           0   \n",
      "2       AL  2010   42880.0   89794.0   43563.0   88001.0        0           0   \n",
      "3       MO  2010   55637.0  106395.0   60740.0  115708.0        0           0   \n",
      "4       NC  2010  114845.0  226709.0  101963.0  201396.0        0           0   \n",
      "...    ...   ...       ...       ...       ...       ...      ...         ...   \n",
      "1066    WA  2009   77278.0  147288.0   72538.0  136914.0        0           0   \n",
      "1067    TX  2009  204851.0  421684.0  163893.0  328137.0        0           0   \n",
      "1068    OR  2009   44892.0   78364.0   41180.0   74673.0        0           0   \n",
      "1069    NV  2009   44554.0   82221.0   47379.0   91828.0        0           0   \n",
      "1070    TN  2009   70156.0  139051.0   64116.0  127939.0        0           0   \n",
      "\n",
      "      Fire  Flood  ...  Snow  Storm  Terrorism  Tornado  Tsunami  Typhoon  \\\n",
      "0        0      1  ...     0      1          0        1        0        0   \n",
      "1        1      0  ...     0      1          0        0        0        0   \n",
      "2        0      0  ...     0      1          0        0        0        0   \n",
      "3        0      0  ...     0      1          0        0        0        0   \n",
      "4        0      0  ...     0      1          0        0        0        0   \n",
      "...    ...    ...  ...   ...    ...        ...      ...      ...      ...   \n",
      "1066     2      1  ...     0      0          0        0        0        0   \n",
      "1067     1      0  ...     0      0          0        0        0        0   \n",
      "1068     1      0  ...     0      0          0        0        0        0   \n",
      "1069     1      0  ...     0      0          0        0        0        0   \n",
      "1070     0      0  ...     0      1          0        0        0        0   \n",
      "\n",
      "      Volcano  Water  Winter  Total disasters  \n",
      "0           0      0       1                0  \n",
      "1           0      0       0                0  \n",
      "2           0      0       0                0  \n",
      "3           0      0       0                0  \n",
      "4           0      0       0                0  \n",
      "...       ...    ...     ...              ...  \n",
      "1066        0      0       0                0  \n",
      "1067        0      0       0                0  \n",
      "1068        0      0       0                0  \n",
      "1069        0      0       0                0  \n",
      "1070        0      0       0                0  \n",
      "\n",
      "[1071 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "joinpath = r'../code/data/Disasters/State_Disasters_by_Year.csv'\n",
    "datajoin = pd.read_csv(joinpath)\n",
    "\n",
    "# join data with datajoin to create final dataset\n",
    "for rowjoin in datajoin.iterrows():\n",
    "    # loop over every row in datajoin\n",
    "    join_state = rowjoin[1][1]\n",
    "    join_start_year = rowjoin[1][3]\n",
    "    join_disaster = rowjoin[1][2]\n",
    "\n",
    "    # loop over every row in data to check for match\n",
    "    for rowdata in data.iterrows():\n",
    "        state = rowdata[1][0]\n",
    "        year = rowdata[1][1]\n",
    "\n",
    "        assert(len(rowdata[1]) == 25)\n",
    "\n",
    "        drought = int(rowdata[1][7])\n",
    "        earthquake = int(rowdata[1][8])\n",
    "        fire = int(rowdata[1][9])\n",
    "        flood = int(rowdata[1][10])\n",
    "        human_cause = int(rowdata[1][11])\n",
    "        hurricane = int(rowdata[1][12])\n",
    "        ice = int(rowdata[1][13])\n",
    "        mud_landslide = int(rowdata[1][14])\n",
    "        other = int(rowdata[1][15])\n",
    "        snow = int(rowdata[1][16])\n",
    "        storm = int(rowdata[1][17])\n",
    "        terrorism = int(rowdata[1][18])\n",
    "        tornado = int(rowdata[1][19])\n",
    "        tsunami = int(rowdata[1][20])\n",
    "        typhoon = int(rowdata[1][21])\n",
    "        volcano = int(rowdata[1][22])\n",
    "        water = int(rowdata[1][23])\n",
    "        winter = int(rowdata[1][24])\n",
    "\n",
    "\n",
    "        if (state == join_state) and (year == join_start_year):\n",
    "            if join_disaster == \"Drought\":\n",
    "                drought += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Drought\"] = drought\n",
    "            elif join_disaster == \"Earthquake\":\n",
    "                earthquake += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Earthquake\"] = earthquake\n",
    "          \n",
    "            elif join_disaster == \"Fire\":\n",
    "                fire += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Fire\"] = fire\n",
    "              \n",
    "            elif join_disaster == \"Flood\":\n",
    "                flood += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Flood\"] = flood\n",
    "             \n",
    "            elif join_disaster == \"Human Cause\":\n",
    "                human_cause += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Human Cause\"] = human_cause\n",
    "               \n",
    "            elif join_disaster == \"Hurricane\":\n",
    "                hurricane += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Hurricane\"] = hurricane\n",
    "              \n",
    "            elif join_disaster == \"Ice\":\n",
    "                ice += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Ice\"] = ice\n",
    "              \n",
    "            elif join_disaster == \"Mud/Landslide\":\n",
    "                mud_landslide += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Mud/Landslide\"] = mud_landslide\n",
    "                \n",
    "            elif join_disaster == \"Other\":\n",
    "                other += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Other\"] = other\n",
    "               \n",
    "            elif join_disaster == \"Snow\":\n",
    "                snow += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Snow\"] = snow\n",
    "                \n",
    "            elif join_disaster == \"Storm\":\n",
    "                storm += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Storm\"] = storm\n",
    "               \n",
    "            elif join_disaster == \"Terrorism\":\n",
    "                terrorism += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Terrorism\"] = terrorism\n",
    "            \n",
    "            elif join_disaster == \"Tornado\":\n",
    "                tornado += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Tornado\"] = tornado\n",
    "             \n",
    "            elif join_disaster == \"Tsunami\":\n",
    "                tsunami += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Tsunami\"] = tsunami\n",
    "             \n",
    "            elif join_disaster == \"Typhoon\":\n",
    "                typhoon += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Earthquake\"] = earthquake\n",
    "             \n",
    "            elif join_disaster == \"Volcano\":\n",
    "                volcano += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Volcano\"] = volcano\n",
    "            \n",
    "            elif join_disaster == \"Water\":\n",
    "                water += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Water\"] = water\n",
    "             \n",
    "            elif join_disaster == \"Winter\":\n",
    "                winter += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Winter\"] = winter\n",
    "              \n",
    "            else:\n",
    "                print(\"else\")\n",
    "            \n",
    "\n",
    "# save changes csv\n",
    "data.to_csv('./data/Total/Neural_Network_Input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# training data\n",
    "path = '../code/data/Total/Neural_Network_Training.csv'\n",
    "training_data = pd.read_csv(path/'Neural_Network_Training.csv')\n",
    "training_data.pop(\"Unnamed: 0\")\n",
    "# testing data\n",
    "testing_data = pd.read_csv('../code/data/Total/Neural_Network_Testing.csv')\n",
    "testing_data.pop(\"Unnamed: 0\")\n",
    "\n",
    "dls = TabularDataLoaders.from_csv('../code/data/Total/Neural_Network_Training.csv',\n",
    "                                    )\n",
    "\n",
    "\n",
    "# learn\n",
    "# learn = tabular_learner(data, layers=[1000, 200, 15], metric=accuracy, emb_drop=0.1, callback_fns=ShowGraph)\n",
    "\n",
    "# dependent variables\n",
    "dependent_var1 = 'NOR(I)'\n",
    "dependent_var2 = 'NOE(I)'\n",
    "dependent_var3 = 'NOR(O)'\n",
    "dependent_var4 = 'NOE(O)'\n",
    "\n",
    "# categorical columns\n",
    "category_names = ['Drought','Earthquake','Fire','Flood','Human Cause','Hurricane','Ice','Mud/Landslide','Other','Snow','Storm','Terrorism','Tornado','Tsunami','Typhoon','Volcano','Water','Winter','Total disasters']\n",
    "\n",
    "# test tabularlist\n",
    "test = TabularDataLoaders.from_df(testing_data,cat_names=category_names)\n",
    "print(len(test))\n",
    "\n",
    "# train bunch\n",
    "data = (TabularDataLoaders.from_df(training_data, path='.', cat_names=category_names)\n",
    "                        .split_by_rand_pct(valid_pct = 0.1, seed = 42)\n",
    "                        .label_from_df(cols = dep_var, label_cls = FloatList, log = True )\n",
    "                        .add_test(test)\n",
    "                        .databunch())\n",
    "\n",
    "# learn\n",
    "learn = tabular_learner(data, layers=[200,100], metrics=rmse)\n",
    "\n",
    "# learning rate\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()\n",
    "\n",
    "# fit model with learning rate\n",
    "learn.fit_one_cycle(15, max_lr =1e-01)\n",
    "\n",
    "# get predictions\n",
    "preds, targets = learn.get_preds(DatasetType.Test)\n",
    "labels = [np.exp(p[0].data.item()) for p in preds]\n",
    "\n",
    "# create final csv\n",
    "prediction = pd.DataFrame({'Id': test_id, 'NOR(1)': labels})\n",
    "prediction.to_csv('prediction.csv', index=False)\n",
    "prediction.head()\n",
    "\n",
    "\n",
    "# learn\n",
    "learn = tabular_learner(data)\n",
    "\n",
    "# model\n",
    "emb_szs = [(4,2), (17,8)]\n",
    "m = TabularModel(emb_szs, n_cont=2, out_sz=2, layers=[200,100]).eval()\n",
    "x_cat = torch.tensor([[2,12]]).long()\n",
    "x_cont = torch.tensor([[0.7633, -0.1887]]).float()\n",
    "out = m(x_cat, x_cont)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drought</th>\n",
       "      <th>Earthquake</th>\n",
       "      <th>Fire</th>\n",
       "      <th>Flood</th>\n",
       "      <th>Human Cause</th>\n",
       "      <th>Hurricane</th>\n",
       "      <th>Ice</th>\n",
       "      <th>Mud/Landslide</th>\n",
       "      <th>Other</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Storm</th>\n",
       "      <th>Terrorism</th>\n",
       "      <th>Tornado</th>\n",
       "      <th>Tsunami</th>\n",
       "      <th>Typhoon</th>\n",
       "      <th>Volcano</th>\n",
       "      <th>Water</th>\n",
       "      <th>Winter</th>\n",
       "      <th>Total disasters</th>\n",
       "      <th>NOE(I)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.730297</td>\n",
       "      <td>1.015757</td>\n",
       "      <td>-0.793886</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>-0.464858</td>\n",
       "      <td>1.295218</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.133298</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.494748</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.422421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>0.430869</td>\n",
       "      <td>-0.331232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.730297</td>\n",
       "      <td>0.431349</td>\n",
       "      <td>-0.604436</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>0.977257</td>\n",
       "      <td>-0.404755</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.426553</td>\n",
       "      <td>-0.492032</td>\n",
       "      <td>-0.645585</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>2.307068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>0.112269</td>\n",
       "      <td>-0.311220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.730297</td>\n",
       "      <td>-1.029671</td>\n",
       "      <td>0.342814</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>0.089802</td>\n",
       "      <td>0.728560</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.426553</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>-1.785918</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.259951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>-1.162133</td>\n",
       "      <td>-0.313805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.364358</td>\n",
       "      <td>0.365148</td>\n",
       "      <td>-1.029671</td>\n",
       "      <td>1.100614</td>\n",
       "      <td>3.007926</td>\n",
       "      <td>-0.464858</td>\n",
       "      <td>-0.971413</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.426553</td>\n",
       "      <td>2.585785</td>\n",
       "      <td>-1.659215</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.942324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>-0.397492</td>\n",
       "      <td>-0.314892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.365148</td>\n",
       "      <td>1.307961</td>\n",
       "      <td>-0.793886</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>-0.575789</td>\n",
       "      <td>1.295218</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.693148</td>\n",
       "      <td>0.607188</td>\n",
       "      <td>0.621451</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.942324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>1.380131</td>\n",
       "      <td>0.813189</td>\n",
       "      <td>-0.310541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.730297</td>\n",
       "      <td>-0.445263</td>\n",
       "      <td>2.047864</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>-0.242994</td>\n",
       "      <td>-0.404755</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.693148</td>\n",
       "      <td>1.266721</td>\n",
       "      <td>0.368044</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-1.104793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>0.749469</td>\n",
       "      <td>-0.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.365148</td>\n",
       "      <td>-1.321875</td>\n",
       "      <td>-0.414986</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>-0.575789</td>\n",
       "      <td>-0.971413</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.426553</td>\n",
       "      <td>-0.272188</td>\n",
       "      <td>-0.518882</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.422421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.364358</td>\n",
       "      <td>1.380131</td>\n",
       "      <td>-1.353293</td>\n",
       "      <td>-0.311496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.730297</td>\n",
       "      <td>-1.175774</td>\n",
       "      <td>1.858414</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>-0.575789</td>\n",
       "      <td>-0.404755</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.426553</td>\n",
       "      <td>-1.151564</td>\n",
       "      <td>-1.405807</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.422421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>1.380131</td>\n",
       "      <td>-1.417013</td>\n",
       "      <td>-0.314334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.365148</td>\n",
       "      <td>1.600165</td>\n",
       "      <td>-0.225536</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>-0.575789</td>\n",
       "      <td>-0.971413</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.426553</td>\n",
       "      <td>-0.272188</td>\n",
       "      <td>-0.012067</td>\n",
       "      <td>4.364358</td>\n",
       "      <td>-1.104793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>-0.312711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>1.460593</td>\n",
       "      <td>0.577451</td>\n",
       "      <td>-1.172786</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>0.311666</td>\n",
       "      <td>0.161902</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>4.052253</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.621451</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.422421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>1.068070</td>\n",
       "      <td>3.348535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.677140</td>\n",
       "      <td>0.076640</td>\n",
       "      <td>0.076640</td>\n",
       "      <td>0.269297</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.488360</td>\n",
       "      <td>0.088914</td>\n",
       "      <td>0.088914</td>\n",
       "      <td>0.288731</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.081004</td>\n",
       "      <td>0.102553</td>\n",
       "      <td>0.102553</td>\n",
       "      <td>0.309861</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.648512</td>\n",
       "      <td>0.113463</td>\n",
       "      <td>0.113463</td>\n",
       "      <td>0.325387</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.480139</td>\n",
       "      <td>0.123945</td>\n",
       "      <td>0.123945</td>\n",
       "      <td>0.339845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.639474</td>\n",
       "      <td>0.135469</td>\n",
       "      <td>0.135469</td>\n",
       "      <td>0.353647</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.529683</td>\n",
       "      <td>0.147046</td>\n",
       "      <td>0.147046</td>\n",
       "      <td>0.367202</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.641764</td>\n",
       "      <td>0.163222</td>\n",
       "      <td>0.163222</td>\n",
       "      <td>0.385421</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.682299</td>\n",
       "      <td>0.179180</td>\n",
       "      <td>0.179180</td>\n",
       "      <td>0.404991</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.756029</td>\n",
       "      <td>0.186519</td>\n",
       "      <td>0.186519</td>\n",
       "      <td>0.410515</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.736750</td>\n",
       "      <td>0.197744</td>\n",
       "      <td>0.197744</td>\n",
       "      <td>0.425309</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.741217</td>\n",
       "      <td>0.201593</td>\n",
       "      <td>0.201593</td>\n",
       "      <td>0.425819</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.611330</td>\n",
       "      <td>0.216070</td>\n",
       "      <td>0.216070</td>\n",
       "      <td>0.444741</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.502529</td>\n",
       "      <td>0.219129</td>\n",
       "      <td>0.219129</td>\n",
       "      <td>0.450994</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.473545</td>\n",
       "      <td>0.224265</td>\n",
       "      <td>0.224265</td>\n",
       "      <td>0.457902</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.427164</td>\n",
       "      <td>0.207020</td>\n",
       "      <td>0.207020</td>\n",
       "      <td>0.435366</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.343727</td>\n",
       "      <td>0.205814</td>\n",
       "      <td>0.205814</td>\n",
       "      <td>0.435145</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.263020</td>\n",
       "      <td>0.200548</td>\n",
       "      <td>0.200548</td>\n",
       "      <td>0.429839</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.188967</td>\n",
       "      <td>0.200221</td>\n",
       "      <td>0.200221</td>\n",
       "      <td>0.430018</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.128318</td>\n",
       "      <td>0.192075</td>\n",
       "      <td>0.192075</td>\n",
       "      <td>0.407191</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.073386</td>\n",
       "      <td>0.186757</td>\n",
       "      <td>0.186757</td>\n",
       "      <td>0.377507</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.028415</td>\n",
       "      <td>0.190567</td>\n",
       "      <td>0.190567</td>\n",
       "      <td>0.348791</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.977882</td>\n",
       "      <td>0.204473</td>\n",
       "      <td>0.204473</td>\n",
       "      <td>0.389969</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.932799</td>\n",
       "      <td>0.248318</td>\n",
       "      <td>0.248318</td>\n",
       "      <td>0.468998</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.888898</td>\n",
       "      <td>0.330078</td>\n",
       "      <td>0.330078</td>\n",
       "      <td>0.532361</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.857449</td>\n",
       "      <td>0.483215</td>\n",
       "      <td>0.483215</td>\n",
       "      <td>0.600720</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.823915</td>\n",
       "      <td>0.721848</td>\n",
       "      <td>0.721848</td>\n",
       "      <td>0.696259</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.789741</td>\n",
       "      <td>1.097212</td>\n",
       "      <td>1.097212</td>\n",
       "      <td>0.836399</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.757471</td>\n",
       "      <td>1.629518</td>\n",
       "      <td>1.629518</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.730923</td>\n",
       "      <td>2.036630</td>\n",
       "      <td>2.036630</td>\n",
       "      <td>1.059485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.705848</td>\n",
       "      <td>2.682183</td>\n",
       "      <td>2.682183</td>\n",
       "      <td>1.207387</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.688789</td>\n",
       "      <td>3.184165</td>\n",
       "      <td>3.184165</td>\n",
       "      <td>1.238398</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.668608</td>\n",
       "      <td>3.615098</td>\n",
       "      <td>3.615098</td>\n",
       "      <td>1.282712</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.644490</td>\n",
       "      <td>4.377795</td>\n",
       "      <td>4.377795</td>\n",
       "      <td>1.388780</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.620491</td>\n",
       "      <td>4.922959</td>\n",
       "      <td>4.922959</td>\n",
       "      <td>1.493727</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.604008</td>\n",
       "      <td>6.165203</td>\n",
       "      <td>6.165203</td>\n",
       "      <td>1.658885</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.586759</td>\n",
       "      <td>7.624292</td>\n",
       "      <td>7.624292</td>\n",
       "      <td>1.835477</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.573289</td>\n",
       "      <td>9.314614</td>\n",
       "      <td>9.314614</td>\n",
       "      <td>1.954821</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.562785</td>\n",
       "      <td>9.529013</td>\n",
       "      <td>9.529013</td>\n",
       "      <td>2.013768</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.550997</td>\n",
       "      <td>9.144192</td>\n",
       "      <td>9.144192</td>\n",
       "      <td>1.945635</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.534543</td>\n",
       "      <td>11.074197</td>\n",
       "      <td>11.074197</td>\n",
       "      <td>2.171962</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.520774</td>\n",
       "      <td>14.990021</td>\n",
       "      <td>14.990021</td>\n",
       "      <td>2.521402</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.508091</td>\n",
       "      <td>18.731197</td>\n",
       "      <td>18.731197</td>\n",
       "      <td>2.836205</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.496846</td>\n",
       "      <td>21.473640</td>\n",
       "      <td>21.473640</td>\n",
       "      <td>3.057880</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.483661</td>\n",
       "      <td>19.801332</td>\n",
       "      <td>19.801332</td>\n",
       "      <td>2.890170</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.470023</td>\n",
       "      <td>19.760914</td>\n",
       "      <td>19.760914</td>\n",
       "      <td>2.836139</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.458837</td>\n",
       "      <td>20.515816</td>\n",
       "      <td>20.515816</td>\n",
       "      <td>2.836254</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.451100</td>\n",
       "      <td>19.544455</td>\n",
       "      <td>19.544455</td>\n",
       "      <td>2.820570</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.438375</td>\n",
       "      <td>19.258371</td>\n",
       "      <td>19.258371</td>\n",
       "      <td>2.768225</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.427137</td>\n",
       "      <td>20.722166</td>\n",
       "      <td>20.722166</td>\n",
       "      <td>2.806648</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.415642</td>\n",
       "      <td>21.506115</td>\n",
       "      <td>21.506115</td>\n",
       "      <td>2.824137</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.406365</td>\n",
       "      <td>27.786434</td>\n",
       "      <td>27.786434</td>\n",
       "      <td>3.167049</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.398359</td>\n",
       "      <td>30.565006</td>\n",
       "      <td>30.565006</td>\n",
       "      <td>3.255118</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.391515</td>\n",
       "      <td>36.548714</td>\n",
       "      <td>36.548714</td>\n",
       "      <td>3.543314</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.384371</td>\n",
       "      <td>46.418430</td>\n",
       "      <td>46.418430</td>\n",
       "      <td>3.899316</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.375692</td>\n",
       "      <td>48.397388</td>\n",
       "      <td>48.397388</td>\n",
       "      <td>3.940088</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.368467</td>\n",
       "      <td>58.868839</td>\n",
       "      <td>58.868839</td>\n",
       "      <td>4.278437</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.362760</td>\n",
       "      <td>66.130432</td>\n",
       "      <td>66.130432</td>\n",
       "      <td>4.540245</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.354027</td>\n",
       "      <td>68.851883</td>\n",
       "      <td>68.851883</td>\n",
       "      <td>4.592458</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.346020</td>\n",
       "      <td>77.167900</td>\n",
       "      <td>77.167900</td>\n",
       "      <td>4.874667</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.337275</td>\n",
       "      <td>86.884560</td>\n",
       "      <td>86.884560</td>\n",
       "      <td>5.188991</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.329098</td>\n",
       "      <td>110.473824</td>\n",
       "      <td>110.473824</td>\n",
       "      <td>5.825151</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.321051</td>\n",
       "      <td>130.604645</td>\n",
       "      <td>130.604645</td>\n",
       "      <td>6.330466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.313290</td>\n",
       "      <td>144.663757</td>\n",
       "      <td>144.663757</td>\n",
       "      <td>6.576611</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.305557</td>\n",
       "      <td>149.717041</td>\n",
       "      <td>149.717041</td>\n",
       "      <td>6.640763</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.299457</td>\n",
       "      <td>165.932449</td>\n",
       "      <td>165.932449</td>\n",
       "      <td>6.948119</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.294680</td>\n",
       "      <td>206.453354</td>\n",
       "      <td>206.453354</td>\n",
       "      <td>7.759173</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.288240</td>\n",
       "      <td>207.180298</td>\n",
       "      <td>207.180298</td>\n",
       "      <td>7.727969</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.283223</td>\n",
       "      <td>258.351471</td>\n",
       "      <td>258.351471</td>\n",
       "      <td>8.628160</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.280499</td>\n",
       "      <td>262.325897</td>\n",
       "      <td>262.325897</td>\n",
       "      <td>8.666589</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>308.154266</td>\n",
       "      <td>308.154266</td>\n",
       "      <td>9.395243</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.272064</td>\n",
       "      <td>338.185547</td>\n",
       "      <td>338.185547</td>\n",
       "      <td>9.792136</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.268326</td>\n",
       "      <td>377.686371</td>\n",
       "      <td>377.686371</td>\n",
       "      <td>10.346911</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.263620</td>\n",
       "      <td>433.381683</td>\n",
       "      <td>433.381683</td>\n",
       "      <td>11.002703</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.257524</td>\n",
       "      <td>536.465332</td>\n",
       "      <td>536.465332</td>\n",
       "      <td>12.147757</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.252383</td>\n",
       "      <td>619.548584</td>\n",
       "      <td>619.548584</td>\n",
       "      <td>13.059465</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.251394</td>\n",
       "      <td>739.526428</td>\n",
       "      <td>739.526428</td>\n",
       "      <td>14.166898</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.246290</td>\n",
       "      <td>855.861084</td>\n",
       "      <td>855.861084</td>\n",
       "      <td>15.181913</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.243382</td>\n",
       "      <td>960.065613</td>\n",
       "      <td>960.065613</td>\n",
       "      <td>16.075506</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.241735</td>\n",
       "      <td>949.915649</td>\n",
       "      <td>949.915649</td>\n",
       "      <td>16.032713</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.238778</td>\n",
       "      <td>1074.021851</td>\n",
       "      <td>1074.021851</td>\n",
       "      <td>17.041513</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.233770</td>\n",
       "      <td>1016.286072</td>\n",
       "      <td>1016.286072</td>\n",
       "      <td>16.552639</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.230399</td>\n",
       "      <td>1103.097290</td>\n",
       "      <td>1103.097290</td>\n",
       "      <td>17.182871</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.227132</td>\n",
       "      <td>1264.172729</td>\n",
       "      <td>1264.172729</td>\n",
       "      <td>18.326637</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.224317</td>\n",
       "      <td>1299.115112</td>\n",
       "      <td>1299.115112</td>\n",
       "      <td>18.598677</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.219843</td>\n",
       "      <td>1307.639771</td>\n",
       "      <td>1307.639771</td>\n",
       "      <td>18.625210</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.215136</td>\n",
       "      <td>1508.976562</td>\n",
       "      <td>1508.976562</td>\n",
       "      <td>20.000460</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.210597</td>\n",
       "      <td>1736.899292</td>\n",
       "      <td>1736.899292</td>\n",
       "      <td>21.468819</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.208910</td>\n",
       "      <td>1920.854980</td>\n",
       "      <td>1920.854980</td>\n",
       "      <td>22.583385</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.205835</td>\n",
       "      <td>2141.184570</td>\n",
       "      <td>2141.184570</td>\n",
       "      <td>23.758450</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.201258</td>\n",
       "      <td>2694.886230</td>\n",
       "      <td>2694.886230</td>\n",
       "      <td>26.617847</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.196662</td>\n",
       "      <td>2991.706055</td>\n",
       "      <td>2991.706055</td>\n",
       "      <td>27.951591</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.194816</td>\n",
       "      <td>3047.080566</td>\n",
       "      <td>3047.080566</td>\n",
       "      <td>28.181089</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.190746</td>\n",
       "      <td>3195.064453</td>\n",
       "      <td>3195.064453</td>\n",
       "      <td>28.806952</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.187512</td>\n",
       "      <td>3580.235596</td>\n",
       "      <td>3580.235596</td>\n",
       "      <td>30.439087</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.187290</td>\n",
       "      <td>3306.318848</td>\n",
       "      <td>3306.318848</td>\n",
       "      <td>29.313419</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.183912</td>\n",
       "      <td>3254.028809</td>\n",
       "      <td>3254.028809</td>\n",
       "      <td>29.069622</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.183708</td>\n",
       "      <td>3594.561279</td>\n",
       "      <td>3594.561279</td>\n",
       "      <td>30.529579</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.183021</td>\n",
       "      <td>4440.606934</td>\n",
       "      <td>4440.606934</td>\n",
       "      <td>33.881306</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.181233</td>\n",
       "      <td>4850.246094</td>\n",
       "      <td>4850.246094</td>\n",
       "      <td>35.363132</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drought</th>\n",
       "      <th>Earthquake</th>\n",
       "      <th>Fire</th>\n",
       "      <th>Flood</th>\n",
       "      <th>Human Cause</th>\n",
       "      <th>Hurricane</th>\n",
       "      <th>Ice</th>\n",
       "      <th>Mud/Landslide</th>\n",
       "      <th>Other</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Storm</th>\n",
       "      <th>Terrorism</th>\n",
       "      <th>Tornado</th>\n",
       "      <th>Tsunami</th>\n",
       "      <th>Typhoon</th>\n",
       "      <th>Volcano</th>\n",
       "      <th>Water</th>\n",
       "      <th>Winter</th>\n",
       "      <th>Total disasters</th>\n",
       "      <th>NOE(I)</th>\n",
       "      <th>NOE(I)_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.730297</td>\n",
       "      <td>0.139145</td>\n",
       "      <td>-0.983336</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>0.422598</td>\n",
       "      <td>-0.404755</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.426553</td>\n",
       "      <td>1.046877</td>\n",
       "      <td>1.001562</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-1.104793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>0.494589</td>\n",
       "      <td>-0.317415</td>\n",
       "      <td>0.588901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.730297</td>\n",
       "      <td>-0.299161</td>\n",
       "      <td>-0.036086</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>0.200734</td>\n",
       "      <td>0.161902</td>\n",
       "      <td>4.364358</td>\n",
       "      <td>0.133298</td>\n",
       "      <td>-1.151564</td>\n",
       "      <td>0.368044</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.259951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>1.380131</td>\n",
       "      <td>-0.078892</td>\n",
       "      <td>-0.312437</td>\n",
       "      <td>-139.591553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.730297</td>\n",
       "      <td>-1.321875</td>\n",
       "      <td>-0.036086</td>\n",
       "      <td>3.007926</td>\n",
       "      <td>-0.242994</td>\n",
       "      <td>-0.971413</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.426553</td>\n",
       "      <td>-0.711876</td>\n",
       "      <td>-0.645585</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>0.942324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>-1.353293</td>\n",
       "      <td>-0.316481</td>\n",
       "      <td>-0.363678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.218218</td>\n",
       "      <td>1.460593</td>\n",
       "      <td>1.454063</td>\n",
       "      <td>-0.793886</td>\n",
       "      <td>-0.316624</td>\n",
       "      <td>-0.575789</td>\n",
       "      <td>-0.404755</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.426553</td>\n",
       "      <td>-0.052344</td>\n",
       "      <td>1.128266</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-1.104793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.690066</td>\n",
       "      <td>0.430869</td>\n",
       "      <td>-0.312716</td>\n",
       "      <td>0.907187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Neural Network -> fastai\n",
    "https://docs.fast.ai/tabular.core.html\n",
    "\n",
    "All:\n",
    "- code/data/Total/Neural_Network_Training.csv\n",
    "\n",
    "Training Data; \n",
    "- code/data/Total/Neural_Network_Training.csv\n",
    "\n",
    "Testing Data:\n",
    "- code/data/Total/Neural_Network_Testing.csv\n",
    "\n",
    "Input: \n",
    "- Disasters (17)\n",
    "- State -cat-names\n",
    "- Year -cont-names\n",
    "\n",
    "Output:\n",
    "- NOR(I): Number of returns (inflow)\n",
    "- NOE(I): Number of exemptions (outflow)\n",
    "- NOR(O): Number of returns (inflow)\n",
    "- NOE(O): Number of exemptions (outflow)\n",
    "\n",
    "1. Did we split up the testing and training data in the right way?\n",
    "2. Is there enough data to go through the neural network? Is this what is breaking our neural network?\n",
    "3. How do we get all the outputs to be predicted? (currently only NOE(I))\n",
    "4. Should we normalize the data like we did?\n",
    "5. Is tabular learning the right way to get the results we want with the given data?\n",
    "\"\"\"\n",
    "\n",
    "from fastai import *\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "# import data\n",
    "# df = pd.read_csv('../code/data/Total/Neural_Network_Training.csv')\n",
    "df = pd.read_csv('../code/data/Total/Total_Input_Training.csv')\n",
    "df.head()\n",
    "\n",
    "# normalize outputs\n",
    "normalized_df = (df-df.mean())/df.std()\n",
    "normalized_df = normalized_df.fillna(0)\n",
    "\n",
    "# data loader\n",
    "# dls = TabularDataLoaders.from_csv('../code/data/Total/Neural_Network_Training.csv', \n",
    "#                                     path = '../code/data/Total/Neural_Network_Training.csv',\n",
    "#                                     y_names = 'NOE(I)', \n",
    "#                                     cont_names = ['Drought','Earthquake','Fire','Flood','Human Cause','Hurricane','Ice','Mud/Landslide','Other','Snow','Storm','Terrorism','Tornado','Tsunami','Typhoon','Volcano','Water','Winter','Total disasters'],\n",
    "#                                     procs = [FillMissing, Normalize])\n",
    "\n",
    "splits = RandomSplitter(valid_pct=0.2)(range_of(normalized_df))\n",
    "to = TabularPandas(normalized_df, procs=[FillMissing],\n",
    "                                cont_names = ['Drought','Earthquake','Fire','Flood','Human Cause','Hurricane','Ice','Mud/Landslide','Other','Snow','Storm','Terrorism','Tornado','Tsunami','Typhoon','Volcano','Water','Winter','Total disasters'],\n",
    "                                y_names='NOE(I)',\n",
    "                                splits = splits)\n",
    "\n",
    "# print(to.xs.iloc[:-1])\n",
    "\n",
    "dls = to.dataloaders(bs=10)\n",
    "print(dls.show_batch())\n",
    "\n",
    "learn = tabular_learner(dls, metrics=[mse,mae])\n",
    "learn.fit_one_cycle(100)\n",
    "\n",
    "learn.show_results()\n",
    "\n",
    "# for b in dls.train:\n",
    "#     print(b[2])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.corr of      Drought  Earthquake      Fire     Flood  Human Cause  Hurricane  \\\n",
       "0  -0.218218   -0.730297 -1.175773  1.858414    -0.316624  -0.575789   \n",
       "1  -0.218218    2.556039 -1.029671  1.858414    -0.316624  -0.242994   \n",
       "2  -0.218218    0.365148 -1.321875 -0.414986    -0.316624  -0.575789   \n",
       "3  -0.218218   -0.730297 -0.445263  2.047864    -0.316624  -0.242994   \n",
       "4  -0.218218   -0.730297 -0.299161 -0.036086    -0.316624   0.200734   \n",
       "5  -0.218218    0.365148  1.307961 -0.793886    -0.316624  -0.575789   \n",
       "6  -0.218218    1.460593  0.869655 -0.414986    -0.316624  -0.242994   \n",
       "7  -0.218218   -0.730297  0.139145 -0.983336    -0.316624   0.422598   \n",
       "8  -0.218218    1.460593  1.454063 -0.793886    -0.316624  -0.575789   \n",
       "9  -0.218218   -0.730297  0.285247 -0.414986    -0.316624   0.089802   \n",
       "10 -0.218218    0.365148 -0.006957  0.532264    -0.316624  -0.242994   \n",
       "11 -0.218218   -0.730297 -1.029671  0.342814    -0.316624   0.089802   \n",
       "12  4.364358    0.365148 -1.029671  1.100614     3.007926  -0.464858   \n",
       "13 -0.218218   -0.730297 -1.321875 -0.036086     3.007926  -0.242994   \n",
       "14 -0.218218   -0.730297 -1.321875  0.721714    -0.316624  -0.575789   \n",
       "15 -0.218218   -0.730297  0.431349 -0.604436    -0.316624   0.977257   \n",
       "16 -0.218218    0.365148  1.600165 -0.225536    -0.316624  -0.575789   \n",
       "17 -0.218218    1.460593  0.577451 -1.172786    -0.316624   0.311666   \n",
       "18 -0.218218   -0.730297  0.285247 -0.793886    -0.316624   3.972419   \n",
       "19 -0.218218   -0.730297  1.015757 -0.983336    -0.316624  -0.464858   \n",
       "20 -0.218218   -0.730297  1.015757 -0.793886    -0.316624  -0.464858   \n",
       "\n",
       "         Ice  Mud/Landslide    NOE(I)    NOE(O)  ...  Terrorism   Tornado  \\\n",
       "0  -0.404755      -0.218218 -0.314334 -0.314584  ...  -0.218218 -0.422421   \n",
       "1  -0.971413      -0.218218 -0.311526 -0.314614  ...  -0.218218  1.624696   \n",
       "2  -0.971413      -0.218218 -0.311496 -0.313535  ...  -0.218218 -0.422421   \n",
       "3  -0.404755      -0.218218 -0.314500 -0.315242  ...  -0.218218 -1.104793   \n",
       "4   0.161902       4.364358 -0.312437 -0.314050  ...  -0.218218  0.259951   \n",
       "5   1.295218      -0.218218 -0.310541 -0.312299  ...  -0.218218  0.942324   \n",
       "6   2.428533      -0.218218 -0.327224 -0.325235  ...  -0.218218  0.942324   \n",
       "7  -0.404755      -0.218218 -0.317415 -0.315594  ...  -0.218218 -1.104793   \n",
       "8  -0.404755      -0.218218 -0.312716 -0.308223  ...  -0.218218 -1.104793   \n",
       "9   1.295218      -0.218218 -0.310725 -0.312961  ...  -0.218218 -1.104793   \n",
       "10 -0.404755      -0.218218 -0.324283 -0.322344  ...  -0.218218  0.259951   \n",
       "11  0.728560      -0.218218 -0.313805 -0.320389  ...  -0.218218  0.259951   \n",
       "12 -0.971413      -0.218218 -0.314892 -0.316536  ...  -0.218218  0.942324   \n",
       "13 -0.971413      -0.218218 -0.316481 -0.318252  ...  -0.218218  0.942324   \n",
       "14 -0.971413      -0.218218  2.628027  2.702200  ...  -0.218218 -0.422421   \n",
       "15 -0.404755      -0.218218 -0.311220 -0.312914  ...  -0.218218  2.307068   \n",
       "16 -0.971413      -0.218218 -0.312711 -0.313907  ...   4.364358 -1.104793   \n",
       "17  0.161902      -0.218218  3.348535  3.287716  ...  -0.218218 -0.422421   \n",
       "18 -0.404755      -0.218218 -0.306560 -0.304924  ...  -0.218218  0.259951   \n",
       "19  1.295218      -0.218218 -0.302462 -0.305139  ...  -0.218218 -1.104793   \n",
       "20  1.295218      -0.218218 -0.331232 -0.329174  ...  -0.218218 -0.422421   \n",
       "\n",
       "    Total disasters  Tsunami  Typhoon  Unnamed: 0  Volcano     Water  \\\n",
       "0         -1.417013      0.0      0.0         0.0      0.0 -0.218218   \n",
       "1         -0.970972      0.0      0.0         0.0      0.0 -0.218218   \n",
       "2         -1.353293      0.0      0.0         0.0      0.0  4.364358   \n",
       "3          0.749469      0.0      0.0         0.0      0.0 -0.218218   \n",
       "4         -0.078892      0.0      0.0         0.0      0.0 -0.218218   \n",
       "5          0.813189      0.0      0.0         0.0      0.0 -0.218218   \n",
       "6         -0.078892      0.0      0.0         0.0      0.0 -0.218218   \n",
       "7          0.494589      0.0      0.0         0.0      0.0 -0.218218   \n",
       "8          0.430869      0.0      0.0         0.0      0.0 -0.218218   \n",
       "9          0.685749      0.0      0.0         0.0      0.0 -0.218218   \n",
       "10         0.685749      0.0      0.0         0.0      0.0 -0.218218   \n",
       "11        -1.162133      0.0      0.0         0.0      0.0 -0.218218   \n",
       "12        -0.397492      0.0      0.0         0.0      0.0 -0.218218   \n",
       "13        -1.353293      0.0      0.0         0.0      0.0 -0.218218   \n",
       "14        -1.417013      0.0      0.0         0.0      0.0 -0.218218   \n",
       "15         0.112269      0.0      0.0         0.0      0.0 -0.218218   \n",
       "16        -0.015171      0.0      0.0         0.0      0.0 -0.218218   \n",
       "17         1.068070      0.0      0.0         0.0      0.0 -0.218218   \n",
       "18         2.406191      0.0      0.0         0.0      0.0 -0.218218   \n",
       "19         0.367149      0.0      0.0         0.0      0.0 -0.218218   \n",
       "20         0.430869      0.0      0.0         0.0      0.0 -0.218218   \n",
       "\n",
       "      Winter      Year  \n",
       "0   1.380131 -1.611646  \n",
       "1  -0.690066 -1.289317  \n",
       "2   1.380131 -0.966988  \n",
       "3  -0.690066 -0.644658  \n",
       "4   1.380131 -0.322329  \n",
       "5   1.380131  0.000000  \n",
       "6   1.380131  0.322329  \n",
       "7  -0.690066  0.644658  \n",
       "8  -0.690066  0.966988  \n",
       "9  -0.690066  1.289317  \n",
       "10  1.380131  1.611646  \n",
       "11 -0.690066 -1.450481  \n",
       "12 -0.690066 -1.128152  \n",
       "13 -0.690066 -0.805823  \n",
       "14 -0.690066 -0.483494  \n",
       "15 -0.690066 -0.161165  \n",
       "16 -0.690066  0.161165  \n",
       "17 -0.690066  0.483494  \n",
       "18 -0.690066  0.805823  \n",
       "19  1.380131  1.128152  \n",
       "20 -0.690066  1.450481  \n",
       "\n",
       "[21 rows x 26 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.corr\n",
    "# plot this - heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

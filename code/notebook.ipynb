{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disaster Types\n",
    "- Year\n",
    "- State\n",
    "- Households Inflow (Number of Returns)\n",
    "- Households Outflow (Number of Returns)\n",
    "- Individuals Inflow (Number of Exemptions)\n",
    "- Individuals Outflow (Number of Exemptions)\n",
    "- Chemical\n",
    "- Dam/Levee Break\n",
    "- Drought\n",
    "- Earthquake\n",
    "- Fire\n",
    "- Flood\n",
    "- Human Cause\n",
    "- Hurricane\n",
    "- Ice\n",
    "- Mud/Landslide\n",
    "- Other\n",
    "- Snow\n",
    "- Storm\n",
    "- Terrorism\n",
    "- Tornado\n",
    "- Tsunami\n",
    "- Typhoon\n",
    "- Volcano\n",
    "- Water\n",
    "- Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Folders Setup\n",
    "\n",
    "code \n",
    "    notebook.ipynb\n",
    "    data\n",
    "        Disasters\n",
    "            FEMA_dataset.csv\n",
    "        StateMigration\n",
    "            1990to1991StateMigration\n",
    "                 1990to1991StateMigrationInflow\n",
    "                     Alabama91in.xls\n",
    "                     Alaska91in.xls\n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91in.xls\n",
    "                     Wyoming91in.xls \n",
    "                 1990to1991StateMigrationOutflow\n",
    "                     Alabama91Out.xls\n",
    "                     Alaska91Out.xls \n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91Out.xls\n",
    "                     Wyoming91Out.xls \n",
    "            .\n",
    "            .\n",
    "            .\n",
    "            2008to2009StateMigration\n",
    "            2009to2010StateMigration\n",
    "            2010to2011StateMigration \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode:\n",
    "\n",
    "### FEMA Dataset Pre-processing (Neely)\n",
    "1. Create new FEMA_dataset with columns \n",
    "    - contains Year, State, Disaster Type\n",
    "2. Name file \"State_Disasters_by_Year\"\n",
    "\n",
    "### StateMigration Data Pre-Processing (Ben)\n",
    "1. Convert all datasets in StateMigration from .xls into .csv files\n",
    "2. Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\" - assign I if from inflow and O if from outflow - from every state file.\n",
    "3. Extract \"State\" and \"Year\" from every file\n",
    "4. Create file with \"State\" (from file name), \"Year\" (from file name), \"Number_of_Returns_I\", \"Number_of_Exemptions_I\", \"Number_of_Returns_I\" and \"Number_of_Exemptions_O\"\n",
    "5. Name file \"State Migration by Year\"\n",
    "\n",
    "### Merge Datasets (Both)\n",
    "1. Merge datasets on common attributes \"Year\" and \"State\"\n",
    "2. Name dataset \"State_Migration_and_Disasters_by_Year\"\n",
    "\n",
    "#### Train and Testing\n",
    "1. Create training and testing datasets\n",
    "    Questions: How should we split training and testing data?\n",
    "2. Create Neural Network models\n",
    "    Input: Year, State, Disaster Type\n",
    "    Output: Migration Inflow (Household/Individual), Migration Outflow (Household/Individual)\n",
    "3. Put training and testing through the Neural Network models.\n",
    "4. Evaluate which models are the most effective.\n",
    "---\n",
    "Data Augmentation\n",
    "Synthetic Data\n",
    "Use svm or decision tree -- skleant -- and compare against a neural network\n",
    "could use all data for training and all data for validation - not this is a faulty practice in \n",
    "20% distribution of state \n",
    "\n",
    "\n",
    "```\n",
    "read_file = pd.read_excel (\"Test.xlsx\")\n",
    " \n",
    "# Write the dataframe object\n",
    "# into csv file\n",
    "read_file.to_csv (\"Test.csv\",\n",
    "                  index = None,\n",
    "                  header=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding imports\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import xlrd\n",
    "import csv\n",
    "#from fastai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEMA Dataset Pre-processing\n",
    "\n",
    "Creates State_Disasters_by_Year.csv with:\n",
    "- State\n",
    "- Disaster Type\n",
    "- Start Year\n",
    "- End Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMA Dataset Preprocessing\n",
    "\n",
    "# copy original FEMA dataset to new file\n",
    "original = r'../code/data/Disasters/FEMA_dataset.csv'\n",
    "new = r'../code/data/Disasters/State_Disasters_by_Year.csv'\n",
    "shutil.copyfile(original, new)\n",
    "\n",
    "# read csv file\n",
    "data = pd.read_csv('~/code/data/Disasters/State_Disasters_by_Year.csv')\n",
    "\n",
    "# delete irrelevant rows\n",
    "data.pop('Declaration Number')\n",
    "data.pop('Declaration Type')\n",
    "data.pop('Declaration Date')\n",
    "data.pop('County')\n",
    "data.pop('Disaster Title')\n",
    "data.pop('Close Date')\n",
    "data.pop('Individual Assistance Program')\n",
    "data.pop('Public Assistance Program')\n",
    "data.pop('Hazard Mitigation Program')\n",
    "data.pop('Individuals & Households Program')\n",
    "\n",
    "# extract years\n",
    "data['Start Year'] = pd.DatetimeIndex(data['Start Date']).year\n",
    "data['End Year'] = pd.DatetimeIndex(data['End Date']).year\n",
    "\n",
    "# delete start and end dates\n",
    "data.pop('Start Date')\n",
    "data.pop('End Date')\n",
    "\n",
    "print(data)\n",
    "\n",
    "# save changes csv\n",
    "data.to_csv('../code/data/Disasters/State_Disasters_by_Year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting State Migration data from .xls to .csv\n",
    "\n",
    "Converting all datasets in StateMigration from .xls to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all files in StateMigration folder from .xls to .csv\n",
    "\n",
    "# create list of xls files\n",
    "xls_list = glob.glob(\"/Users/ben/Desktop/climate-displacement/code/data/StateMigration/*/*/*.xls\")\n",
    "\n",
    "# replace xls \n",
    "for xls_file in xls_list:\n",
    "    \n",
    "    wb = xlrd.open_workbook(xls_file)\n",
    "    sh = wb.sheet_by_index(0)\n",
    "    csv_file = open(xls_file[0:-3]+'csv', \"w\")\n",
    "    wr = csv.writer(csv_file, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    for rownum in range(sh.nrows):\n",
    "        wr.writerow(sh.row_values(rownum))\n",
    "        \n",
    "    csv_file.close()\n",
    "    \n",
    "    # remove .xls files\n",
    "    os.remove(xls_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More data wrangling - StateMigration dataset\n",
    "- Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\"\n",
    "- Assign \"I\" if from inflow and \"O\" if from outflow - for every state file\n",
    "- Extract \"State\" and \"Year\" from every file\n",
    "- Create file with \"State\" (from file name), \"Year\" (from file name), \"Number of Returns_I\", \"Number of Exemptions I\", \"Number of Returns O\" and \"Number of Exemptions O\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "1. create output path in repository for the merged StateMigration dataset\n",
    "\n",
    "2. read each csv file in the StateMigration folder, and for each file, \n",
    "    - d\n",
    "3. check the csv files to make sure they are the intended data\n",
    "4. remove the original csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new file location for merged StateMigration dataset\n",
    "output_path = r'../code/data/StateMigration/State_Migrations_by_Year.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output file at output_path\n",
    "output = open(output_path, \"w\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [State, Year, NOR(I), NOE(I), NOR(O), NOE(O)]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# create empty DataFrame object\n",
    "df = pd.DataFrame()\n",
    "df.insert(0,'State', '')\n",
    "df.insert(1,'Year', '')\n",
    "df.insert(2,'NOR(I)', '')\n",
    "df.insert(3,'NOE(I)', '')\n",
    "df.insert(4,'NOR(O)', '')\n",
    "df.insert(5,'NOE(O)', '')\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of \"state initial keys\" with multiple \"values\"\n",
    "# run each segment through dictionary, and convert into state initial\n",
    "stateDict = {\n",
    "    \"AL\":['Alabama', 'al', 'AL', 'alab', 'Alab'],\n",
    "    \"AK\":['Alaska', 'ak,' 'AK', 'alas', 'Alas'],\n",
    "    \"AZ\":['Arizona', 'az', 'AZ', 'ariz', 'Ariz'],\n",
    "    \"AR\":['Arkansas', 'ar', 'AR', 'arka', 'Arka'],\n",
    "    \"CA\":['California', 'ca', 'CA', 'cali', 'Cali'],\n",
    "    \"CO\":['Colorado', 'co', 'CO', 'colo', 'Colo'],\n",
    "    \"CT\":['Connecticut', 'ct', 'CT', 'conn', 'Conn'],\n",
    "    \"DE\":['Delaware', 'de', 'DE', 'dela', 'Dela'],\n",
    "    \"DC\":['DistrictofColumbia', 'Districtofcolumbia', 'District of Columbia', 'dc', 'DC', 'dist', 'Dist', 'DiCo', 'dico'],\n",
    "    \"FL\":['Florida', 'fl', 'FL', 'flor', 'Flor'],\n",
    "    \"GA\":['Georgia', 'ga', 'GA', 'geor', 'Geor'],\n",
    "    \"HI\":['Hawaii', 'hi', 'HI', 'hawa', 'Hawa'],\n",
    "    \"ID\":['Idaho', 'id', 'ID', 'idah', 'Idah'],\n",
    "    \"IL\":['Illinois', 'il', 'IL', 'illi', 'Illi'],\n",
    "    \"IN\":['Indiana', 'in', 'IN', 'indi', 'Indi'],\n",
    "    \"IA\":['Iowa', 'ia', 'IA', 'iowa'],\n",
    "    \"KS\":['Kansas', 'ks', 'KS', 'kans', 'Kans'],\n",
    "    \"KY\":['Kentucky', 'ky', 'KY', 'kent', 'Kent'],\n",
    "    \"LA\":['Louisiana', 'la', 'LA', 'loui', 'Loui'],\n",
    "    \"MA\":['Massachusetts', 'ma', 'MA', 'mass', 'Mass'],\n",
    "    \"MD\":['Maryland', 'md', 'MD', 'mary', 'Mary'],\n",
    "    \"ME\":['Maine', 'me', 'ME', 'main', 'Main'],\n",
    "    \"MI\":['Michigan', 'mi', 'MI', 'mich', 'Mich'],\n",
    "    \"MN\":['Minnesota', 'mn', 'MN', 'minn', 'Minn'],\n",
    "    \"MO\":['Missouri', 'mo', 'MO', 'Miso', 'miso'],\n",
    "    \"MS\":['Mississippi', 'ms', 'MS', 'Misi', 'misi', 'miss', 'Miss'],\n",
    "    \"MT\":['Montana', 'mt', 'MT', 'mont', 'Mont'],\n",
    "    \"NC\":['North Carolina', 'NorthCarolina', 'nc', 'NC', 'NoCa', 'noca'],\n",
    "    \"ND\":['North Dakota', 'NorthDakota', 'nd', 'ND', 'NoDa', 'noda'],\n",
    "    \"NE\":['Nebraska', 'ne', 'NE', 'Nebr', 'nrbt', 'nebr'],\n",
    "    \"NH\":['New Hampshire', 'NewHampshire', 'nh', 'NH', 'NeHa', 'neha', 'newh'],\n",
    "    \"NJ\":['New Jersey', 'NewJersey', 'nj', 'NJ', 'NeJe', 'neje', 'newj'],\n",
    "    \"NM\":['New Mexico', 'NewMexico', 'nm', 'NM', 'NeMe', 'neme', 'newm'],\n",
    "    \"NV\":['Nevada', 'nv', 'NV', 'Neva', 'neva'],\n",
    "    \"NY\":['New York', 'NewYork', 'ny', 'NY', 'newy', 'NeYo', 'neyo'],\n",
    "    \"OH\":['Ohio', 'oh', 'OH', 'ohio'],\n",
    "    \"OK\":['Oklahoma', 'ok', 'OK', 'okla', 'Okla'],\n",
    "    \"OR\":['Oregon', 'or', 'OR', 'oreg', 'Oreg'],\n",
    "    \"PA\":['Pennsylvania', 'pa', 'PA', 'penn', 'Penn'],\n",
    "    \"RI\":['Rhode Island', 'RhodeIsland', 'ra', 'RA', 'Rhls', 'rhod', 'Rhod'],\n",
    "    \"SC\":['South Carolina', 'SouthCarolina', 'sc', 'SC', 'SoCa', 'soca'],\n",
    "    \"SD\":['South Dakota', 'SouthDakota', 'sd', 'SD', 'SoDa', 'soda'],\n",
    "    \"TN\":['Tennessee', 'tn', 'TN', 'Tenn', 'tenn'],\n",
    "    \"TX\":['Texas', 'tx', 'TX', 'texa', 'Texa'],\n",
    "    \"UT\":['Utah', 'ut', 'UT', 'utah'],\n",
    "    \"VA\":['Virginia', 'va', 'VA', 'virg', 'Virg', 'vrg'],\n",
    "    \"WA\":['Washington', 'wa', 'WA', 'wash', 'Wash'],\n",
    "    \"WI\":['Wisconsin', 'wi', 'WI', 'wisc', 'Wisc'],\n",
    "    \"WV\":['West Virginia', 'WestVirginia', 'wv', 'WV', 'west', 'wevi', 'wvir'],\n",
    "    \"WY\":['Wyoming', 'wy', 'WY', 'wyom', 'Wyom']    \n",
    "}\n",
    "\n",
    "def get_key(val):\n",
    "    for key, valueList in stateDict.items():\n",
    "         for value in valueList:\n",
    "            if val == value:\n",
    "             return key\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007to2008StateMigrationInflow\n",
      "Illinois0708in.csv\n",
      "    2007- 2008 State To State Migration Flows - INFLOW,Unnamed: 1,Unnamed: 2,Unnamed: 3,Unnamed: 4,Unnamed: 5\n",
      "0     (Aggregate Money Amounts Are In Thousands),,,,,                                                        \n",
      "1                                               ,,,,,                                                        \n",
      "2                        TO:,17-Illinois,,,,AGGREGATE                                                        \n",
      "3                                       ,,,,,ADJUSTED                                                        \n",
      "4                        ,,,NUMBER OF,NUMBER OF,GROSS                                                        \n",
      "..                                                ...                                                        \n",
      "57                           23,ME,MAINE,180,282,6753                                                        \n",
      "58                    44,RI,RHODE ISLAND,166,268,8087                                                        \n",
      "59                       10,DE,DELAWARE,165,341,11794                                                        \n",
      "60                         56,WY,WYOMING,141,264,5348                                                        \n",
      "61                         50,VT,VERMONT,122,188,7631                                                        \n",
      "\n",
      "[62 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# read csv file\n",
    "csv_list = glob.glob(\"/Users/ben/Desktop/climate-displacement/code/data/StateMigration/*/*/*.csv\")\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    # os.path.split returns a list of (head, tail) where head is the parent directories \n",
    "    # and tail is the filename and extension\n",
    "    temp = os.path.split(csv_file)\n",
    "    temp2 = os.path.split(temp[0])\n",
    "    \n",
    "    # get file name and parent folder from temp, temp2 respectively\n",
    "    filename = temp[1]\n",
    "    parentfile = temp2[1]\n",
    "    \n",
    "    # extract state, year, and inflow/outflow\n",
    "    # three different naming conventions in the StateMigration dataset\n",
    "    # 1) [State][Year1Year2 e.g. (0708)][in/out]\n",
    "    # 2) [State][Year2 e.g. 91][In/Out]\n",
    "    # 3) [Year1Year2 like 1)]inmig[in/out][state INITIAL e.g. AL]\n",
    "    # 4) [first 4 letters of State][Year2][in/ot]\n",
    "    # 5) s9[last digits of Year1, Year2 e.g. 56][state INITIAL][ir/or]\n",
    "    # 6) same as 4) but with extra \"r\" at the end\n",
    "    \n",
    "    # naming convention 1: used for years 2004-2009\n",
    "    # naming convention 2: used for years 1990-1993\n",
    "    # naming convention 3: used for years 2009-2010\n",
    "    # naming convention 4: used for years 1993-1995, 1996-2000, 2001-2004\n",
    "    # naming convention 5: used for years 1995-1996\n",
    "    # naming convention 6: used for years 2000-2001\n",
    "    \n",
    "    # extract inflow/outflow, year using parentfile, and state using filename\n",
    "    \n",
    "    print(filename)\n",
    "    data = pd.read_csv(csv_file)\n",
    "    print(data)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close output csv file\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f) and filename.endswith('.txt'):\n",
    "        read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "    read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "\n",
    "location = \"/Users/neely/Desktop/climate-displacement/code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow\"\n",
    "placement = \"./Users/neely/Desktop/climate-displacement/code/data/StateMigration\"\n",
    "\n",
    "for file in glob.glob(\"*.xls\"):\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir('../code/data/StateMigration/') if isfile(join('../code/data/StateMigration/', f))]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'untar_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8505139c4a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# path to dataset here (./Users/neely/Desktop/climate-displacement/code/data/StateMigration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntar_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURLs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'untar_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Neural Network -> fastai\n",
    "\n",
    "# path to dataset here (./Users/neely/Desktop/climate-displacement/code/data/StateMigration)\n",
    "path = untar_data(URLs.MNIST)\n",
    "path.ls()\n",
    "\n",
    "# data loader\n",
    "dls = ImageDataLoaders.from_folder(path, train=\"training\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Displacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disaster Types\n",
    "- Year\n",
    "- State\n",
    "- Households Inflow (Number of Returns)\n",
    "- Households Outflow (Number of Returns)\n",
    "- Individuals Inflow (Number of Exemptions)\n",
    "- Individuals Outflow (Number of Exemptions)\n",
    "- Chemical\n",
    "- Dam/Levee Break\n",
    "- Drought\n",
    "- Earthquake\n",
    "- Fire\n",
    "- Flood\n",
    "- Human Cause\n",
    "- Hurricane\n",
    "- Ice\n",
    "- Mud/Landslide\n",
    "- Other\n",
    "- Snow\n",
    "- Storm\n",
    "- Terrorism\n",
    "- Tornado\n",
    "- Tsunami\n",
    "- Typhoon\n",
    "- Volcano\n",
    "- Water\n",
    "- Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFolders Setup\\n\\ncode \\n    notebook.ipynb\\n    data\\n        Disasters\\n            FEMA_dataset.csv\\n        StateMigration\\n            1990to1991StateMigration\\n                 1990to1991StateMigrationInflow\\n                     Alabama91in.xls\\n                     Alaska91in.xls\\n                     .\\n                     .\\n                     .\\n                     Wisconsin91in.xls\\n                     Wyoming91in.xls \\n                 1990to1991StateMigrationOutflow\\n                     Alabama91Out.xls\\n                     Alaska91Out.xls \\n                     .\\n                     .\\n                     .\\n                     Wisconsin91Out.xls\\n                     Wyoming91Out.xls \\n            .\\n            .\\n            .\\n            2008to2009StateMigration\\n            2009to2010StateMigration\\n            2010to2011StateMigration \\n'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Folders Setup\n",
    "\n",
    "code \n",
    "    notebook.ipynb\n",
    "    data\n",
    "        Disasters\n",
    "            FEMA_dataset.csv\n",
    "        StateMigration\n",
    "            1990to1991StateMigration\n",
    "                 1990to1991StateMigrationInflow\n",
    "                     Alabama91in.xls\n",
    "                     Alaska91in.xls\n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91in.xls\n",
    "                     Wyoming91in.xls \n",
    "                 1990to1991StateMigrationOutflow\n",
    "                     Alabama91Out.xls\n",
    "                     Alaska91Out.xls \n",
    "                     .\n",
    "                     .\n",
    "                     .\n",
    "                     Wisconsin91Out.xls\n",
    "                     Wyoming91Out.xls \n",
    "            .\n",
    "            .\n",
    "            .\n",
    "            2008to2009StateMigration\n",
    "            2009to2010StateMigration\n",
    "            2010to2011StateMigration \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode:\n",
    "\n",
    "### FEMA Dataset Pre-processing (Neely)\n",
    "1. Create new FEMA_dataset with columns \n",
    "    - contains Year, State, Disaster Type\n",
    "2. Name file \"State_Disasters_by_Year\"\n",
    "\n",
    "### StateMigration Data Pre-Processing (Ben)\n",
    "1. Convert all datasets in StateMigration from .xls into .csv files\n",
    "2. Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\" - assign I if from inflow and O if from outflow - from every state file.\n",
    "3. Extract \"State\" and \"Year\" from every file\n",
    "4. Create file with \"State\" (from file name), \"Year\" (from file name), \"Number_of_Returns_I\", \"Number_of_Exemptions_I\", \"Number_of_Returns_I\" and \"Number_of_Exemptions_O\"\n",
    "5. Name file \"State Migration by Year\"\n",
    "\n",
    "### Merge Datasets (Both)\n",
    "1. Merge datasets on common attributes \"Year\" and \"State\"\n",
    "2. Name dataset \"State_Migration_and_Disasters_by_Year\"\n",
    "\n",
    "#### Train and Testing\n",
    "1. Create training and testing datasets\n",
    "    Questions: How should we split training and testing data?\n",
    "2. Create Neural Network models\n",
    "    Input: Year, State, Disaster Type\n",
    "    Output: Migration Inflow (Household/Individual), Migration Outflow (Household/Individual)\n",
    "3. Put training and testing through the Neural Network models.\n",
    "4. Evaluate which models are the most effective.\n",
    "---\n",
    "Data Augmentation\n",
    "Synthetic Data\n",
    "Use svm or decision tree -- skleant -- and compare against a neural network\n",
    "could use all data for training and all data for validation - not this is a faulty practice in \n",
    "20% distribution of state \n",
    "\n",
    "\n",
    "```\n",
    "read_file = pd.read_excel (\"Test.xlsx\")\n",
    " \n",
    "# Write the dataframe object\n",
    "# into csv file\n",
    "read_file.to_csv (\"Test.csv\",\n",
    "                  index = None,\n",
    "                  header=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding imports\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "#import xlrd\n",
    "import csv\n",
    "import numpy as np\n",
    "#from fastai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEMA Dataset Pre-processing\n",
    "\n",
    "Creates State_Disasters_by_Year.csv with:\n",
    "- State\n",
    "- Disaster Type\n",
    "- Start Year\n",
    "- End Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      State Disaster Type  Start Year  End Year\n",
      "0        TX        Winter        1989    1989.0\n",
      "1        TX        Winter        1989    1989.0\n",
      "2        TX        Winter        1989    1989.0\n",
      "3        TX        Winter        1989    1989.0\n",
      "4        TX        Winter        1989    1989.0\n",
      "...     ...           ...         ...       ...\n",
      "36785    CA         Storm        2017    2017.0\n",
      "36786    CA         Storm        2017    2017.0\n",
      "36787    CA         Storm        2017    2017.0\n",
      "36788    CA         Storm        2017    2017.0\n",
      "36789    CA         Storm        2017    2017.0\n",
      "\n",
      "[36790 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# FEMA Dataset Preprocessing\n",
    "\n",
    "# copy original FEMA dataset to new file\n",
    "original = r'../code/data/Disasters/FEMA_dataset.csv'\n",
    "new = r'../code/data/Disasters/State_Disasters_by_Year.csv'\n",
    "shutil.copyfile(original, new)\n",
    "\n",
    "# read csv file\n",
    "data = pd.read_csv('../code/data/Disasters/State_Disasters_by_Year.csv')\n",
    "\n",
    "# delete irrelevant rows\n",
    "data.pop('Declaration Number')\n",
    "data.pop('Declaration Type')\n",
    "data.pop('Declaration Date')\n",
    "data.pop('County')\n",
    "data.pop('Disaster Title')\n",
    "data.pop('Close Date')\n",
    "data.pop('Individual Assistance Program')\n",
    "data.pop('Public Assistance Program')\n",
    "data.pop('Hazard Mitigation Program')\n",
    "data.pop('Individuals & Households Program')\n",
    "\n",
    "# extract years\n",
    "data['Start Year'] = pd.DatetimeIndex(data['Start Date']).year\n",
    "data['End Year'] = pd.DatetimeIndex(data['End Date']).year\n",
    "\n",
    "# delete start and end dates\n",
    "data.pop('Start Date')\n",
    "data.pop('End Date')\n",
    "\n",
    "print(data)\n",
    "\n",
    "# save changes csv\n",
    "data.to_csv('../code/data/Disasters/State_Disasters_by_Year.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting State Migration data from .xls to .csv\n",
    "\n",
    "Converting all datasets in StateMigration from .xls to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all files in StateMigration folder from .xls to .csv\n",
    "\n",
    "# create list of xls files\n",
    "xls_list = glob.glob(\"/Users/ben/Desktop/climate-displacement/code/data/StateMigration/*/*/*.xls\")\n",
    "\n",
    "# replace xls \n",
    "for xls_file in xls_list:\n",
    "    \n",
    "    wb = xlrd.open_workbook(xls_file)\n",
    "    sh = wb.sheet_by_index(0)\n",
    "    csv_file = open(xls_file[0:-3]+'csv', \"w\")\n",
    "    wr = csv.writer(csv_file, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    for rownum in range(sh.nrows):\n",
    "        wr.writerow(sh.row_values(rownum))\n",
    "        \n",
    "    csv_file.close()\n",
    "    \n",
    "    # remove .xls files\n",
    "    os.remove(xls_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More data wrangling - StateMigration dataset\n",
    "- Extract \"Total Flow\" row with \"Number of Returns\" and \"Number of Exemptions\"\n",
    "- Assign \"I\" if from inflow and \"O\" if from outflow - for every state file\n",
    "- Extract \"State\" and \"Year\" from every file\n",
    "- Create file with \"State\" (from file name), \"Year\" (from file name), \"Number of Returns_I\", \"Number of Exemptions I\", \"Number of Returns O\" and \"Number of Exemptions O\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "1. create output path in repository for the merged StateMigration dataset\n",
    "\n",
    "2. read each csv file in the StateMigration folder, and for each file:\n",
    "\n",
    "    a. check the csv files to make sure they are the intended data\n",
    "    \n",
    "    b. remove the original csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new file location for merged StateMigration dataset\n",
    "output_path = r'../code/data/StateMigration/State_Migrations_by_Year.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output file at output_path\n",
    "output = open(output_path, \"w\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty DataFrame object\n",
    "df = pd.DataFrame()\n",
    "df.insert(0,'State', '')\n",
    "df.insert(1,'Year', '')\n",
    "df.insert(2,'NOR(I)', '')\n",
    "df.insert(3,'NOE(I)', '')\n",
    "df.insert(4,'NOR(O)', '')\n",
    "df.insert(5,'NOE(O)', '')\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WI\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of \"state initial keys\" with multiple \"values\"\n",
    "# run each segment through dictionary, and convert into state initial\n",
    "stateDict = {\n",
    "    \"AL\":['Alabama', 'al', 'AL', 'alab', 'Alab'],\n",
    "    \"AK\":['Alaska', 'ak', 'AK', 'alas', 'Alas'],\n",
    "    \"AZ\":['Arizona', 'az', 'AZ', 'ariz', 'Ariz'],\n",
    "    \"AR\":['Arkansas', 'ar', 'AR', 'arka', 'Arka', 'aka'],\n",
    "    \"CA\":['California', 'ca', 'CA', 'cali', 'Cali'],\n",
    "    \"CO\":['Colorado', 'co', 'CO', 'colo', 'Colo'],\n",
    "    \"CT\":['Connecticut', 'ct', 'CT', 'conn', 'Conn'],\n",
    "    \"DE\":['Delaware', 'de', 'DE', 'dela', 'Dela'],\n",
    "    \"DC\":['DistrictofColumbia', 'Districtofcolumbia', 'District of Columbia', 'dc', 'DC', 'dist', 'Dist', 'DiCo', 'dico'],\n",
    "    \"FL\":['Florida', 'fl', 'FL', 'flor', 'Flor'],\n",
    "    \"GA\":['Georgia', 'ga', 'GA', 'geor', 'Geor'],\n",
    "    \"HI\":['Hawaii', 'hi', 'HI', 'hawa', 'Hawa'],\n",
    "    \"ID\":['Idaho', 'id', 'ID', 'idah', 'Idah'],\n",
    "    \"IL\":['Illinois', 'il', 'IL', 'illi', 'Illi'],\n",
    "    \"IN\":['Indiana', 'in', 'IN', 'indi', 'Indi'],\n",
    "    \"IA\":['Iowa', 'ia', 'IA', 'iowa'],\n",
    "    \"KS\":['Kansas', 'ks', 'KS', 'kans', 'Kans'],\n",
    "    \"KY\":['Kentucky', 'ky', 'KY', 'kent', 'Kent'],\n",
    "    \"LA\":['Louisiana', 'la', 'LA', 'loui', 'Loui'],\n",
    "    \"MA\":['Massachusetts', 'ma', 'MA', 'mass', 'Mass'],\n",
    "    \"MD\":['Maryland', 'md', 'MD', 'mary', 'Mary'],\n",
    "    \"ME\":['Maine', 'me', 'ME', 'main', 'Main'],\n",
    "    \"MI\":['Michigan', 'mi', 'MI', 'mich', 'Mich'],\n",
    "    \"MN\":['Minnesota', 'mn', 'MN', 'minn', 'Minn'],\n",
    "    \"MO\":['Missouri', 'mo', 'MO', 'Miso', 'miso'],\n",
    "    \"MS\":['Mississippi', 'ms', 'MS', 'Misi', 'misi', 'miss', 'Miss'],\n",
    "    \"MT\":['Montana', 'mt', 'MT', 'mont', 'Mont'],\n",
    "    \"NC\":['North Carolina', 'NorthCarolina', 'nc', 'NC', 'NoCa', 'noca', 'ncar', 'Northcarolina'],\n",
    "    \"ND\":['North Dakota', 'NorthDakota', 'nd', 'ND', 'NoDa', 'noda', 'ndak', 'Northdakota'],\n",
    "    \"NE\":['Nebraska', 'ne', 'NE', 'Nebr', 'nrbt', 'nebr'],\n",
    "    \"NH\":['New Hampshire', 'NewHampshire', 'nh', 'NH', 'NeHa', 'neha', 'newh'],\n",
    "    \"NJ\":['New Jersey', 'NewJersey', 'nj', 'NJ', 'NeJe', 'neje', 'newj', 'Newjersey'],\n",
    "    \"NM\":['New Mexico', 'NewMexico', 'nm', 'NM', 'NeMe', 'neme', 'newm', 'Newmexico'],\n",
    "    \"NV\":['Nevada', 'nv', 'NV', 'Neva', 'neva'],\n",
    "    \"NY\":['New York', 'NewYork', 'ny', 'NY', 'newy', 'NeYo', 'neyo', 'newY','Newyork'],\n",
    "    \"OH\":['Ohio', 'oh', 'OH', 'ohio', 'nhio'],\n",
    "    \"OK\":['Oklahoma', 'ok', 'OK', 'okla', 'Okla'],\n",
    "    \"OR\":['Oregon', 'or', 'OR', 'oreg', 'Oreg', 'oeg'],\n",
    "    \"PA\":['Pennsylvania', 'pa', 'PA', 'penn', 'Penn'],\n",
    "    \"RI\":['Rhode Island', 'RhodeIsland', 'ri', 'RI', 'Rhls', 'rhod', 'Rhod', 'RhIs'],\n",
    "    \"SC\":['South Carolina', 'SouthCarolina', 'sc', 'SC', 'SoCa', 'soca', 'scar', 'Southcarolina'],\n",
    "    \"SD\":['South Dakota', 'SouthDakota', 'sd', 'SD', 'SoDa', 'soda', 'sdak', 'Southdakota'],\n",
    "    \"TN\":['Tennessee', 'tn', 'TN', 'Tenn', 'tenn'],\n",
    "    \"TX\":['Texas', 'tx', 'TX', 'texa', 'Texa'],\n",
    "    \"UT\":['Utah', 'ut', 'UT', 'utah'],\n",
    "    \"VA\":['Virginia', 'va', 'VA', 'virg', 'Virg', 'vrg'],\n",
    "    \"VT\":['Vermont', 'vt', 'VT', 'verm', 'Verm'],\n",
    "    \"WA\":['Washington', 'wa', 'WA', 'wash', 'Wash'],\n",
    "    \"WI\":['Wisconsin', 'wi', 'WI', 'wisc', 'Wisc', 'wiso', 'wsc'],\n",
    "    \"WV\":['West Virginia', 'WestVirginia', 'wv', 'WV', 'west', 'wevi', 'wvir', 'Westvirginia'],\n",
    "    \"WY\":['Wyoming', 'wy', 'WY', 'wyom', 'Wyom']    \n",
    "}\n",
    "\n",
    "def getKey(val):\n",
    "    for key, valueList in stateDict.items():\n",
    "         for value in valueList:\n",
    "            if val == value:\n",
    "                 return key\n",
    "    \n",
    "    return False\n",
    "\n",
    "print(getKey('Wisconsin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read csv file\n",
    "# csv_list = glob.glob(\"/Users/ben/Desktop/climate-displacement/code/data/StateMigration/*/*/*.csv\")\n",
    "csv_list = glob.glob(\"../code/data/StateMigration/*/*/*.csv\")\n",
    "\n",
    "# create list of row_params\n",
    "list_param = []\n",
    "\n",
    "for csv_file in csv_list:\n",
    "    # os.path.split returns a list of (head, tail) where head is the parent directories \n",
    "    # and tail is the filename and extension\n",
    "    temp = os.path.split(csv_file)\n",
    "    temp2 = os.path.split(temp[0])\n",
    "    \n",
    "    # get file name and parent folder from temp, temp2 respectively\n",
    "    filename = temp[1]\n",
    "    parentfile = temp2[1]\n",
    "    \n",
    "    # print (filename, parentfile)\n",
    "    # print (type(filename))\n",
    "    \n",
    "    # extract state, year, and inflow/outflow\n",
    "    # three different naming conventions in the StateMigration dataset\n",
    "    # 1) [State][Year1Year2 e.g. (0708)][in/out]\n",
    "    # 2) [State][Year2 e.g. 91][In/Out]\n",
    "    # 3) [Year1Year2 like 1)]inmig[in/out][state INITIAL e.g. AL]\n",
    "    # 4) [first 4 letters of State][Year2][in/ot]\n",
    "    # 5) s9[last digits of Year1, Year2 e.g. 56][state INITIAL][ir/or]\n",
    "    # 6) same as 4) but with extra \"r\" at the end\n",
    "    \n",
    "    # naming convention 1: used for years 2004-2009\n",
    "    name1 = [2004,2005,2006,2007,2008]\n",
    "    # naming convention 2: used for years 1990-1993\n",
    "    name2 = [1990,1991,1992]\n",
    "    # naming convention 3: used for years 2009-2011\n",
    "    name3 = [2009,2010]\n",
    "    # naming convention 4: used for years 1993-1995, 1996-2000, 2001-2004\n",
    "    name4 = [1993,1994,1996,1997, 1998, 1999, 2001,2002,2003]\n",
    "    # naming convention 5: used for years 1995-1996\n",
    "    name5 = 1995\n",
    "    # naming convention 6: used for years 2000-2001\n",
    "    name6 = 2000\n",
    "    \n",
    "    # extract inflow/outflow, year using parentfile, and state using filename\n",
    "    if parentfile[-6] == 'u':\n",
    "        io = parentfile[-7:]\n",
    "    elif parentfile[-5] == 'n':\n",
    "        io = parentfile[-6:]\n",
    "    year = int(parentfile[0:4])\n",
    "    \n",
    "    #2009to2010StateMigrationInflow\n",
    "    # print(io, year)\n",
    "    \n",
    "    if year in name1:\n",
    "        if io == 'Inflow':\n",
    "            state = filename[:-10]\n",
    "        elif io == 'Outflow':\n",
    "            state = filename[:-11]\n",
    "    elif year in name2:\n",
    "        if io == 'Inflow':\n",
    "            state = filename[:-8]\n",
    "        elif io == 'Outflow':\n",
    "            state = filename[:-9]\n",
    "    elif year in name3:\n",
    "        state = filename[-6:-4]\n",
    "    elif year in name4:\n",
    "        state = filename[:4]\n",
    "        if state == 'vrg9':\n",
    "            state = 'vrg'\n",
    "        elif state == 'vrg0':\n",
    "            state = 'vrg'\n",
    "        elif state == 'az94':\n",
    "            state = 'az'\n",
    "        elif state == 'aka9':\n",
    "            state = 'aka'\n",
    "        elif state == 'wsc9':\n",
    "            state = 'wsc'\n",
    "    elif year == name5:\n",
    "        state = filename[-8:-6]\n",
    "    elif year == name6:\n",
    "        state = filename[:4]\n",
    "        if state == 'vrg0':\n",
    "            state = 'vrg'\n",
    "        elif state == 'oeg0':\n",
    "            state = 'oeg'\n",
    "\n",
    "    si = getKey(state)\n",
    "    if si != False:\n",
    "        row_param = [si, year, io]\n",
    "    # print(row_param)\n",
    "    \n",
    "    # the total flow data in each years are located in different rows and columns.\n",
    "    # type1 - 1990, 1991: located in row 9, columns D and F\n",
    "    type1 = [1990, 1991]\n",
    "    # type2 - 1992-1994, 2004-2006: located in row 9, columns D and E\n",
    "    type2 = [1992,1993,1994,2004,2005,2006]\n",
    "    # type3 - 1995-2003, 2007-2008: located in row 10, columns D and E\n",
    "    type3 = [1995,1996,1997,1998,1999,2000,2001,2002,2003,2007,2008]\n",
    "    # type4 - 2009-2010: located in row 8, columns E and F\n",
    "    type4 = [2009,2010]\n",
    "    \n",
    "    data = pd.read_csv(csv_file)\n",
    "    # print(data)\n",
    "    if si != False:\n",
    "        if year in type1:\n",
    "            # total = data.iloc[7] #7, np.r_[3,5]\n",
    "            totaltemp = data.iat[7,0]\n",
    "            list = totaltemp.split(\",\")\n",
    "            if ((si == \"AL\") and (io == \"Outflow\")):\n",
    "                nor = list[4]\n",
    "                noe = list[6]\n",
    "            else:\n",
    "                nor = list[3]\n",
    "                noe = list[5]\n",
    "        elif year in type2:\n",
    "            # total = data.iloc[7] #7\n",
    "            totaltemp = data.iat[7,0]\n",
    "            #print(totaltemp)\n",
    "            list = totaltemp.split(\",\")\n",
    "            # print(list)\n",
    "            nor = list[3]\n",
    "            noe = list[4]\n",
    "        elif year in type3:\n",
    "            if year == 2003:\n",
    "                nor = data.iat[8,4]\n",
    "                noe = data.iat[8,5]\n",
    "            elif year == 1997:\n",
    "                nor = data.iat[8,4]\n",
    "                noe = data.iat[8,5]\n",
    "            elif year == 2002:\n",
    "                totaltemp = data.iat[8,0]\n",
    "                list = totaltemp.split(\",\")\n",
    "                nor = list[3]\n",
    "                noe = list[4]\n",
    "            else:\n",
    "                totaltemp = data.iat[7,0]\n",
    "                list = totaltemp.split(\",\")\n",
    "                nor = list[3]\n",
    "                noe = list[4]\n",
    "        elif year in type4:\n",
    "            total = data.iloc[6]\n",
    "            nor = data.iat[6,4]\n",
    "            noe = data.iat[6,5]\n",
    "        # print(data)    \n",
    "        # print(total)\n",
    "        # print(nor, noe)\n",
    "        # print(data)\n",
    "        \n",
    "        row_param.append(nor)\n",
    "        row_param.append(noe)\n",
    "        \n",
    "        list_param.append(row_param)\n",
    "        # print(row_param)\n",
    "        # data.shape\n",
    "        # break\n",
    "#print(list_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = []\n",
    "# print(list_param)\n",
    "for row in list_param:\n",
    "    isIOMatch = False\n",
    "\n",
    "    for row2 in current:\n",
    "        if ((row[0] == row2[0]) and (row[1] == row2[1])):\n",
    "            \n",
    "            if row[2] == \"Inflow\": # you have inflow data to add, outflow already exists\n",
    "                row2.insert(3, row[3]) #0-si, 1-year, 2-io, 3-nor, 4-noe\n",
    "                row2.insert(4, row[4])\n",
    "                row2.pop(2)\n",
    "                isIOMatch = True\n",
    "            elif row[2] == \"Outflow\": # you have outflow data to add, inflow already exists\n",
    "                row2.append(row[3])\n",
    "                row2.append(row[4])\n",
    "                row2.pop(2)\n",
    "                isIOMatch = True\n",
    "    if isIOMatch == False:\n",
    "        if row[2] == \"Inflow\": # adding inflow data\n",
    "            newrow = [row[0],row[1],\"Inflow\",row[3],row[4]]\n",
    "        elif (row[2] == \"Outflow\"): # adding outflow data\n",
    "            newrow = [row[0],row[1],\"Outflow\",row[3],row[4]]\n",
    "        current.append(newrow)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     State  Year    NOR(I)    NOE(I)    NOR(O)    NOE(O)\n",
      "0       MN  2010   41038.0   73463.0   45173.0   77893.0\n",
      "1       AZ  2010   83528.0  158038.0   80278.0  162164.0\n",
      "2       AL  2010   42880.0   89794.0   43563.0   88001.0\n",
      "3       MO  2010   55637.0  106395.0   60740.0  115708.0\n",
      "4       NC  2010  114845.0  226709.0  101963.0  201396.0\n",
      "...    ...   ...       ...       ...       ...       ...\n",
      "1066    WA  2009   77278.0  147288.0   72538.0  136914.0\n",
      "1067    TX  2009  204851.0  421684.0  163893.0  328137.0\n",
      "1068    OR  2009   44892.0   78364.0   41180.0   74673.0\n",
      "1069    NV  2009   44554.0   82221.0   47379.0   91828.0\n",
      "1070    TN  2009   70156.0  139051.0   64116.0  127939.0\n",
      "\n",
      "[1071 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(current)\n",
    "data = pd.DataFrame(current, columns=['State', 'Year', 'NOR(I)', 'NOE(I)', 'NOR(O)', 'NOE(O)'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert df into csv?\n",
    "stmg_csv = data.to_csv(output_path)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom os.path import isfile, join\\nonlyfiles = [f for f in listdir('../code/data/StateMigration/') if isfile(join('../code/data/StateMigration/', f))]\\n\""
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f) and filename.endswith('.txt'):\n",
    "        read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "    read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "read_file = pd.read_excel (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.xls')\n",
    "read_file.to_csv (r'../code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow/Alabama91in.csv', index = None, header=True)\n",
    "\n",
    "\n",
    "\n",
    "location = \"/Users/neely/Desktop/climate-displacement/code/data/StateMigration/1990to1991StateMigration/1990to1991StateMigrationInflow\"\n",
    "placement = \"./Users/neely/Desktop/climate-displacement/code/data/StateMigration\"\n",
    "\n",
    "for file in glob.glob(\"*.xls\"):\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir('../code/data/StateMigration/') if isfile(join('../code/data/StateMigration/', f))]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     State  Year    NOR(I)    NOE(I)    NOR(O)    NOE(O)  Drought  Earthquake  \\\n",
      "0       MN  2010   41038.0   73463.0   45173.0   77893.0        0           0   \n",
      "1       AZ  2010   83528.0  158038.0   80278.0  162164.0        0           0   \n",
      "2       AL  2010   42880.0   89794.0   43563.0   88001.0        0           0   \n",
      "3       MO  2010   55637.0  106395.0   60740.0  115708.0        0           0   \n",
      "4       NC  2010  114845.0  226709.0  101963.0  201396.0        0           0   \n",
      "...    ...   ...       ...       ...       ...       ...      ...         ...   \n",
      "1066    WA  2009   77278.0  147288.0   72538.0  136914.0        0           0   \n",
      "1067    TX  2009  204851.0  421684.0  163893.0  328137.0        0           0   \n",
      "1068    OR  2009   44892.0   78364.0   41180.0   74673.0        0           0   \n",
      "1069    NV  2009   44554.0   82221.0   47379.0   91828.0        0           0   \n",
      "1070    TN  2009   70156.0  139051.0   64116.0  127939.0        0           0   \n",
      "\n",
      "      Fire  Flood  ...  Snow  Storm  Terrorism  Tornado  Tsunami  Typhoon  \\\n",
      "0        0      0  ...     0      0          0        0        0        0   \n",
      "1        0      0  ...     0      0          0        0        0        0   \n",
      "2        0      0  ...     0      0          0        0        0        0   \n",
      "3        0      0  ...     0      0          0        0        0        0   \n",
      "4        0      0  ...     0      0          0        0        0        0   \n",
      "...    ...    ...  ...   ...    ...        ...      ...      ...      ...   \n",
      "1066     0      0  ...     0      0          0        0        0        0   \n",
      "1067     0      0  ...     0      0          0        0        0        0   \n",
      "1068     0      0  ...     0      0          0        0        0        0   \n",
      "1069     0      0  ...     0      0          0        0        0        0   \n",
      "1070     0      0  ...     0      0          0        0        0        0   \n",
      "\n",
      "      Volcano  Water  Winter  Total disasters  \n",
      "0           0      0       0                0  \n",
      "1           0      0       0                0  \n",
      "2           0      0       0                0  \n",
      "3           0      0       0                0  \n",
      "4           0      0       0                0  \n",
      "...       ...    ...     ...              ...  \n",
      "1066        0      0       0                0  \n",
      "1067        0      0       0                0  \n",
      "1068        0      0       0                0  \n",
      "1069        0      0       0                0  \n",
      "1070        0      0       0                0  \n",
      "\n",
      "[1071 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Create final dataset\n",
    "\n",
    "1. Copy State_Disaster_by_Year\n",
    "2. Add column for each natural disaster\n",
    "3. Join over common year and state matches\n",
    "4. Increase number of natural disasters if natural disaster of same type already exists in a year\n",
    "\"\"\"\n",
    "\n",
    "# FEMA Dataset Preprocessing\n",
    "\n",
    "# copy data from State_Migration_by_Year.csv\n",
    "original = r'../code/data/StateMigration/State_Migrations_by_Year.csv'\n",
    "new = r'../code/data/Total/Neural_Network_Input.csv'\n",
    "shutil.copyfile(original, new)\n",
    "\n",
    "# read csv file\n",
    "data = pd.read_csv(new)\n",
    "\n",
    "# delete irrelevant rows\n",
    "data.pop('Unnamed: 0')\n",
    "\n",
    "# create climate disaster columns\n",
    "data[\"Drought\"] = 0\n",
    "data[\"Earthquake\"] = 0\n",
    "data[\"Fire\"] = 0\n",
    "data[\"Flood\"] = 0\n",
    "data[\"Human Cause\"] = 0\n",
    "data[\"Hurricane\"] = 0\n",
    "data[\"Ice\"] = 0\n",
    "data[\"Mud/Landslide\"] = 0\n",
    "data[\"Other\"] = 0\n",
    "data[\"Snow\"] = 0\n",
    "data[\"Storm\"] = 0\n",
    "data[\"Terrorism\"] = 0\n",
    "data[\"Tornado\"] = 0\n",
    "data[\"Tsunami\"] = 0\n",
    "data[\"Typhoon\"] = 0\n",
    "data[\"Volcano\"] = 0\n",
    "data[\"Water\"] = 0\n",
    "data [\"Winter\"] = 0\n",
    "data[\"Total disasters\"] = 0\n",
    "\n",
    "print(data)\n",
    "\n",
    "data.to_csv('../code/data/Total/Neural_Network_Input.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "else\n",
      "else\n",
      "else\n",
      "     State  Year    NOR(I)    NOE(I)    NOR(O)    NOE(O)  Drought  Earthquake  \\\n",
      "0       MN  2010   41038.0   73463.0   45173.0   77893.0        0           0   \n",
      "1       AZ  2010   83528.0  158038.0   80278.0  162164.0        0           0   \n",
      "2       AL  2010   42880.0   89794.0   43563.0   88001.0        0           0   \n",
      "3       MO  2010   55637.0  106395.0   60740.0  115708.0        0           0   \n",
      "4       NC  2010  114845.0  226709.0  101963.0  201396.0        0           0   \n",
      "...    ...   ...       ...       ...       ...       ...      ...         ...   \n",
      "1066    WA  2009   77278.0  147288.0   72538.0  136914.0        0           0   \n",
      "1067    TX  2009  204851.0  421684.0  163893.0  328137.0        0           0   \n",
      "1068    OR  2009   44892.0   78364.0   41180.0   74673.0        0           0   \n",
      "1069    NV  2009   44554.0   82221.0   47379.0   91828.0        0           0   \n",
      "1070    TN  2009   70156.0  139051.0   64116.0  127939.0        0           0   \n",
      "\n",
      "      Fire  Flood  ...  Snow  Storm  Terrorism  Tornado  Tsunami  Typhoon  \\\n",
      "0        0      1  ...     0      1          0        1        0        0   \n",
      "1        1      0  ...     0      1          0        0        0        0   \n",
      "2        0      0  ...     0      1          0        0        0        0   \n",
      "3        0      0  ...     0      1          0        0        0        0   \n",
      "4        0      0  ...     0      1          0        0        0        0   \n",
      "...    ...    ...  ...   ...    ...        ...      ...      ...      ...   \n",
      "1066     2      1  ...     0      0          0        0        0        0   \n",
      "1067     1      0  ...     0      0          0        0        0        0   \n",
      "1068     1      0  ...     0      0          0        0        0        0   \n",
      "1069     1      0  ...     0      0          0        0        0        0   \n",
      "1070     0      0  ...     0      1          0        0        0        0   \n",
      "\n",
      "      Volcano  Water  Winter  Total disasters  \n",
      "0           0      0       1                0  \n",
      "1           0      0       0                0  \n",
      "2           0      0       0                0  \n",
      "3           0      0       0                0  \n",
      "4           0      0       0                0  \n",
      "...       ...    ...     ...              ...  \n",
      "1066        0      0       0                0  \n",
      "1067        0      0       0                0  \n",
      "1068        0      0       0                0  \n",
      "1069        0      0       0                0  \n",
      "1070        0      0       0                0  \n",
      "\n",
      "[1071 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "joinpath = r'../code/data/Disasters/State_Disasters_by_Year.csv'\n",
    "datajoin = pd.read_csv(joinpath)\n",
    "\n",
    "# join data with datajoin to create final dataset\n",
    "for rowjoin in datajoin.iterrows():\n",
    "    # loop over every row in datajoin\n",
    "    join_state = rowjoin[1][1]\n",
    "    join_start_year = rowjoin[1][3]\n",
    "    join_disaster = rowjoin[1][2]\n",
    "    #print(join_state)\n",
    "    #print(join_start_year)\n",
    "    #print(join_disaster)\n",
    "\n",
    "    # loop over every row in data to check for match\n",
    "    for rowdata in data.iterrows():\n",
    "        #print(rowdata)\n",
    "        #print(\"outter\")\n",
    "        state = rowdata[1][0]\n",
    "        year = rowdata[1][1]\n",
    "\n",
    "        #print(join_state)\n",
    "        #print(state)\n",
    "        #print(join_start_year)\n",
    "        #print(year)\n",
    "        assert(len(rowdata[1]) == 25)\n",
    "\n",
    "        drought = int(rowdata[1][7])\n",
    "        earthquake = int(rowdata[1][8])\n",
    "        fire = int(rowdata[1][9])\n",
    "        flood = int(rowdata[1][10])\n",
    "        human_cause = int(rowdata[1][11])\n",
    "        hurricane = int(rowdata[1][12])\n",
    "        ice = int(rowdata[1][13])\n",
    "        mud_landslide = int(rowdata[1][14])\n",
    "        other = int(rowdata[1][15])\n",
    "        snow = int(rowdata[1][16])\n",
    "        storm = int(rowdata[1][17])\n",
    "        terrorism = int(rowdata[1][18])\n",
    "        tornado = int(rowdata[1][19])\n",
    "        tsunami = int(rowdata[1][20])\n",
    "        typhoon = int(rowdata[1][21])\n",
    "        volcano = int(rowdata[1][22])\n",
    "        water = int(rowdata[1][23])\n",
    "        winter = int(rowdata[1][24])\n",
    "\n",
    "\n",
    "        if (state == join_state) and (year == join_start_year):\n",
    "            #print(\"Inner\")\n",
    "            if join_disaster == \"Drought\":\n",
    "                drought += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Drought\"] = drought\n",
    "            elif join_disaster == \"Earthquake\":\n",
    "                earthquake += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Earthquake\"] = earthquake\n",
    "          \n",
    "            elif join_disaster == \"Fire\":\n",
    "                fire += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Fire\"] = fire\n",
    "              \n",
    "            elif join_disaster == \"Flood\":\n",
    "                flood += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Flood\"] = flood\n",
    "             \n",
    "            elif join_disaster == \"Human Cause\":\n",
    "                human_cause += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Human Cause\"] = human_cause\n",
    "               \n",
    "            elif join_disaster == \"Hurricane\":\n",
    "                hurricane += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Hurricane\"] = hurricane\n",
    "              \n",
    "            elif join_disaster == \"Ice\":\n",
    "                ice += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Ice\"] = ice\n",
    "              \n",
    "            elif join_disaster == \"Mud/Landslide\":\n",
    "                mud_landslide += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Mud/Landslide\"] = mud_landslide\n",
    "                \n",
    "            elif join_disaster == \"Other\":\n",
    "                other += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Other\"] = other\n",
    "               \n",
    "            elif join_disaster == \"Snow\":\n",
    "                snow += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Snow\"] = snow\n",
    "                \n",
    "            elif join_disaster == \"Storm\":\n",
    "                storm += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Storm\"] = storm\n",
    "               \n",
    "            elif join_disaster == \"Terrorism\":\n",
    "                terrorism += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Terrorism\"] = terrorism\n",
    "            \n",
    "            elif join_disaster == \"Tornado\":\n",
    "                tornado += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Tornado\"] = tornado\n",
    "             \n",
    "            elif join_disaster == \"Tsunami\":\n",
    "                tsunami += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Tsunami\"] = tsunami\n",
    "             \n",
    "            elif join_disaster == \"Typhoon\":\n",
    "                typhoon += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Earthquake\"] = earthquake\n",
    "             \n",
    "            elif join_disaster == \"Volcano\":\n",
    "                volcano += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Volcano\"] = volcano\n",
    "            \n",
    "            elif join_disaster == \"Water\":\n",
    "                water += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Water\"] = water\n",
    "             \n",
    "            elif join_disaster == \"Winter\":\n",
    "                winter += 1\n",
    "                data.loc[rowdata[0]:rowdata[0],\"Winter\"] = winter\n",
    "              \n",
    "            else:\n",
    "                print(\"else\")\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# extract years\n",
    "# data['Start Year'] = pd.DatetimeIndex(data['Start Date']).year\n",
    "# data['End Year'] = pd.DatetimeIndex(data['End Date']).year\n",
    "\n",
    "\n",
    "print(data)\n",
    "\n",
    "# save changes csv\n",
    "\n",
    "data.to_csv('./data/Total/Neural_Network_Input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Neural Network -> fastai\\n\\n# path to dataset here (./Users/neely/Desktop/climate-displacement/code/data/StateMigration)\\npath = untar_data(URLs.MNIST)\\npath.ls()\\n\\n# data loader\\ndls = ImageDataLoaders.from_folder(path, train=\"training\", valid=\"testing\")\\ndls.show_batch()\\n\\n\\n\\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\\nlearn.fine_tune(1)\\n# help(cnn_learner)\\n# ?cnn_learner\\n# \\n'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Neural Network -> fastai\n",
    "\n",
    "# path to dataset here (./Users/neely/Desktop/climate-displacement/code/data/StateMigration)\n",
    "path = untar_data(URLs.MNIST)\n",
    "path.ls()\n",
    "\n",
    "# data loader\n",
    "dls = ImageDataLoaders.from_folder(path, train=\"training\", valid=\"testing\")\n",
    "dls.show_batch()\n",
    "\n",
    "\n",
    "\n",
    "learn = cnn_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.fine_tune(1)\n",
    "# help(cnn_learner)\n",
    "# ?cnn_learner\n",
    "# \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
